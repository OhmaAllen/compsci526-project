{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MIMIC-IV 临床时间序列建模模板\n",
        "\n",
        "此 notebook 提供：\n",
        "- 数据加载（假设 CSV 已导出到 `data/`）\n",
        "- 慢性病患者筛选、人口学与静态特征抽取\n",
        "- 时序数据重采样与缺失值插补\n",
        "- 基线模型（Logistic Regression, Random Forest），ARIMA 示例\n",
        "- 深度学习模型（LSTM, Transformer）训练示例\n",
        "- 解释性示例（SHAP）与注意力可视化提示\n",
        "\n",
        "> **注意**：这是通用模板。请根据你实际的 `chartevents` / `labevents` 字段、ITEMID 列表和计算资源进行调整。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "当前工作目录： /Users/yuchenzhou/Documents/duke/compsci526/final_proj/git_proj/yuchen_testing\n",
            "已将 DATA_DIR 默认设置为: /Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1\n",
            "请确保下列子目录存在并包含 CSV：\n",
            " - hosp/ 例如: hosp/admissions.csv, hosp/patients.csv, hosp/diagnoses_icd.csv, hosp/labevents.csv, hosp/prescriptions.csv\n",
            " - icu/ 例如: icu/chartevents.csv, icu/d_items.csv, icu/icustays.csv\n"
          ]
        }
      ],
      "source": [
        "# 写入本地 MIMIC 目录说明（已将示例路径替换为你的本地路径）\n",
        "import os\n",
        "# 修改为你的 MIMIC 导出目录（包含 hosp/ 和 icu/ 子目录）。\n",
        "DATA_DIR = '/Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1'\n",
        "print('当前工作目录：', os.getcwd())\n",
        "print(f'已将 DATA_DIR 默认设置为: {DATA_DIR}')\n",
        "print('请确保下列子目录存在并包含 CSV：')\n",
        "print(' - hosp/ 例如: hosp/admissions.csv, hosp/patients.csv, hosp/diagnoses_icd.csv, hosp/labevents.csv, hosp/prescriptions.csv')\n",
        "print(' - icu/ 例如: icu/chartevents.csv, icu/d_items.csv, icu/icustays.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "debc67c7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- admissions -> /Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1/hosp/admissions.csv\n",
            "columns: ['subject_id', 'hadm_id', 'admittime', 'dischtime', 'deathtime', 'admission_type', 'admit_provider_id', 'admission_location', 'discharge_location', 'insurance', 'language', 'marital_status', 'race', 'edregtime', 'edouttime', 'hospital_expire_flag']\n",
            "preview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject_id</th>\n",
              "      <th>hadm_id</th>\n",
              "      <th>admittime</th>\n",
              "      <th>dischtime</th>\n",
              "      <th>deathtime</th>\n",
              "      <th>admission_type</th>\n",
              "      <th>admit_provider_id</th>\n",
              "      <th>admission_location</th>\n",
              "      <th>discharge_location</th>\n",
              "      <th>insurance</th>\n",
              "      <th>language</th>\n",
              "      <th>marital_status</th>\n",
              "      <th>race</th>\n",
              "      <th>edregtime</th>\n",
              "      <th>edouttime</th>\n",
              "      <th>hospital_expire_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000032</td>\n",
              "      <td>22595853</td>\n",
              "      <td>2180-05-06 22:23:00</td>\n",
              "      <td>2180-05-07 17:15:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>URGENT</td>\n",
              "      <td>P49AFC</td>\n",
              "      <td>TRANSFER FROM HOSPITAL</td>\n",
              "      <td>HOME</td>\n",
              "      <td>Medicaid</td>\n",
              "      <td>English</td>\n",
              "      <td>WIDOWED</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>2180-05-06 19:17:00</td>\n",
              "      <td>2180-05-06 23:30:00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10000032</td>\n",
              "      <td>22841357</td>\n",
              "      <td>2180-06-26 18:27:00</td>\n",
              "      <td>2180-06-27 18:49:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EW EMER.</td>\n",
              "      <td>P784FA</td>\n",
              "      <td>EMERGENCY ROOM</td>\n",
              "      <td>HOME</td>\n",
              "      <td>Medicaid</td>\n",
              "      <td>English</td>\n",
              "      <td>WIDOWED</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>2180-06-26 15:54:00</td>\n",
              "      <td>2180-06-26 21:31:00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10000032</td>\n",
              "      <td>25742920</td>\n",
              "      <td>2180-08-05 23:44:00</td>\n",
              "      <td>2180-08-07 17:50:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EW EMER.</td>\n",
              "      <td>P19UTS</td>\n",
              "      <td>EMERGENCY ROOM</td>\n",
              "      <td>HOSPICE</td>\n",
              "      <td>Medicaid</td>\n",
              "      <td>English</td>\n",
              "      <td>WIDOWED</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>2180-08-05 20:58:00</td>\n",
              "      <td>2180-08-06 01:44:00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   subject_id   hadm_id            admittime            dischtime  deathtime  \\\n",
              "0    10000032  22595853  2180-05-06 22:23:00  2180-05-07 17:15:00        NaN   \n",
              "1    10000032  22841357  2180-06-26 18:27:00  2180-06-27 18:49:00        NaN   \n",
              "2    10000032  25742920  2180-08-05 23:44:00  2180-08-07 17:50:00        NaN   \n",
              "\n",
              "  admission_type admit_provider_id      admission_location discharge_location  \\\n",
              "0         URGENT            P49AFC  TRANSFER FROM HOSPITAL               HOME   \n",
              "1       EW EMER.            P784FA          EMERGENCY ROOM               HOME   \n",
              "2       EW EMER.            P19UTS          EMERGENCY ROOM            HOSPICE   \n",
              "\n",
              "  insurance language marital_status   race            edregtime  \\\n",
              "0  Medicaid  English        WIDOWED  WHITE  2180-05-06 19:17:00   \n",
              "1  Medicaid  English        WIDOWED  WHITE  2180-06-26 15:54:00   \n",
              "2  Medicaid  English        WIDOWED  WHITE  2180-08-05 20:58:00   \n",
              "\n",
              "             edouttime  hospital_expire_flag  \n",
              "0  2180-05-06 23:30:00                     0  \n",
              "1  2180-06-26 21:31:00                     0  \n",
              "2  2180-08-06 01:44:00                     0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- patients -> /Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1/hosp/patients.csv\n",
            "columns: ['subject_id', 'gender', 'anchor_age', 'anchor_year', 'anchor_year_group', 'dod']\n",
            "preview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject_id</th>\n",
              "      <th>gender</th>\n",
              "      <th>anchor_age</th>\n",
              "      <th>anchor_year</th>\n",
              "      <th>anchor_year_group</th>\n",
              "      <th>dod</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000032</td>\n",
              "      <td>F</td>\n",
              "      <td>52</td>\n",
              "      <td>2180</td>\n",
              "      <td>2014 - 2016</td>\n",
              "      <td>2180-09-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10000048</td>\n",
              "      <td>F</td>\n",
              "      <td>23</td>\n",
              "      <td>2126</td>\n",
              "      <td>2008 - 2010</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10000058</td>\n",
              "      <td>F</td>\n",
              "      <td>33</td>\n",
              "      <td>2168</td>\n",
              "      <td>2020 - 2022</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   subject_id gender  anchor_age  anchor_year anchor_year_group         dod\n",
              "0    10000032      F          52         2180       2014 - 2016  2180-09-09\n",
              "1    10000048      F          23         2126       2008 - 2010         NaN\n",
              "2    10000058      F          33         2168       2020 - 2022         NaN"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- diagnoses_icd -> /Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1/hosp/diagnoses_icd.csv\n",
            "columns: ['subject_id', 'hadm_id', 'seq_num', 'icd_code', 'icd_version']\n",
            "preview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject_id</th>\n",
              "      <th>hadm_id</th>\n",
              "      <th>seq_num</th>\n",
              "      <th>icd_code</th>\n",
              "      <th>icd_version</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000032</td>\n",
              "      <td>22595853</td>\n",
              "      <td>1</td>\n",
              "      <td>5723</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10000032</td>\n",
              "      <td>22595853</td>\n",
              "      <td>2</td>\n",
              "      <td>78959</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10000032</td>\n",
              "      <td>22595853</td>\n",
              "      <td>3</td>\n",
              "      <td>5715</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   subject_id   hadm_id  seq_num  icd_code  icd_version\n",
              "0    10000032  22595853        1      5723            9\n",
              "1    10000032  22595853        2     78959            9\n",
              "2    10000032  22595853        3      5715            9"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- chartevents -> /Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1/icu/chartevents.csv\n",
            "columns: ['subject_id', 'hadm_id', 'stay_id', 'caregiver_id', 'charttime', 'storetime', 'itemid', 'value', 'valuenum', 'valueuom', 'warning']\n",
            "preview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject_id</th>\n",
              "      <th>hadm_id</th>\n",
              "      <th>stay_id</th>\n",
              "      <th>caregiver_id</th>\n",
              "      <th>charttime</th>\n",
              "      <th>storetime</th>\n",
              "      <th>itemid</th>\n",
              "      <th>value</th>\n",
              "      <th>valuenum</th>\n",
              "      <th>valueuom</th>\n",
              "      <th>warning</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000032</td>\n",
              "      <td>29079034</td>\n",
              "      <td>39553978</td>\n",
              "      <td>18704</td>\n",
              "      <td>2180-07-23 12:36:00</td>\n",
              "      <td>2180-07-23 14:45:00</td>\n",
              "      <td>226512</td>\n",
              "      <td>39.4</td>\n",
              "      <td>39.4</td>\n",
              "      <td>kg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10000032</td>\n",
              "      <td>29079034</td>\n",
              "      <td>39553978</td>\n",
              "      <td>18704</td>\n",
              "      <td>2180-07-23 12:36:00</td>\n",
              "      <td>2180-07-23 14:45:00</td>\n",
              "      <td>226707</td>\n",
              "      <td>60</td>\n",
              "      <td>60.0</td>\n",
              "      <td>Inch</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10000032</td>\n",
              "      <td>29079034</td>\n",
              "      <td>39553978</td>\n",
              "      <td>18704</td>\n",
              "      <td>2180-07-23 12:36:00</td>\n",
              "      <td>2180-07-23 14:45:00</td>\n",
              "      <td>226730</td>\n",
              "      <td>152</td>\n",
              "      <td>152.0</td>\n",
              "      <td>cm</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   subject_id   hadm_id   stay_id  caregiver_id            charttime  \\\n",
              "0    10000032  29079034  39553978         18704  2180-07-23 12:36:00   \n",
              "1    10000032  29079034  39553978         18704  2180-07-23 12:36:00   \n",
              "2    10000032  29079034  39553978         18704  2180-07-23 12:36:00   \n",
              "\n",
              "             storetime  itemid value  valuenum valueuom  warning  \n",
              "0  2180-07-23 14:45:00  226512  39.4      39.4       kg        0  \n",
              "1  2180-07-23 14:45:00  226707    60      60.0     Inch        0  \n",
              "2  2180-07-23 14:45:00  226730   152     152.0       cm        0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- labevents -> /Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1/hosp/labevents.csv\n",
            "columns: ['labevent_id', 'subject_id', 'hadm_id', 'specimen_id', 'itemid', 'order_provider_id', 'charttime', 'storetime', 'value', 'valuenum', 'valueuom', 'ref_range_lower', 'ref_range_upper', 'flag', 'priority', 'comments']\n",
            "preview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labevent_id</th>\n",
              "      <th>subject_id</th>\n",
              "      <th>hadm_id</th>\n",
              "      <th>specimen_id</th>\n",
              "      <th>itemid</th>\n",
              "      <th>order_provider_id</th>\n",
              "      <th>charttime</th>\n",
              "      <th>storetime</th>\n",
              "      <th>value</th>\n",
              "      <th>valuenum</th>\n",
              "      <th>valueuom</th>\n",
              "      <th>ref_range_lower</th>\n",
              "      <th>ref_range_upper</th>\n",
              "      <th>flag</th>\n",
              "      <th>priority</th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>10000032</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2704548</td>\n",
              "      <td>50931</td>\n",
              "      <td>P69FQC</td>\n",
              "      <td>2180-03-23 11:51:00</td>\n",
              "      <td>2180-03-23 15:56:00</td>\n",
              "      <td>___</td>\n",
              "      <td>95.0</td>\n",
              "      <td>mg/dL</td>\n",
              "      <td>70.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ROUTINE</td>\n",
              "      <td>IF FASTING, 70-100 NORMAL, &gt;125 PROVISIONAL DI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>10000032</td>\n",
              "      <td>NaN</td>\n",
              "      <td>36092842</td>\n",
              "      <td>51071</td>\n",
              "      <td>P69FQC</td>\n",
              "      <td>2180-03-23 11:51:00</td>\n",
              "      <td>2180-03-23 16:00:00</td>\n",
              "      <td>NEG</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ROUTINE</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>10000032</td>\n",
              "      <td>NaN</td>\n",
              "      <td>36092842</td>\n",
              "      <td>51074</td>\n",
              "      <td>P69FQC</td>\n",
              "      <td>2180-03-23 11:51:00</td>\n",
              "      <td>2180-03-23 16:00:00</td>\n",
              "      <td>NEG</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ROUTINE</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   labevent_id  subject_id  hadm_id  specimen_id  itemid order_provider_id  \\\n",
              "0            1    10000032      NaN      2704548   50931            P69FQC   \n",
              "1            2    10000032      NaN     36092842   51071            P69FQC   \n",
              "2            3    10000032      NaN     36092842   51074            P69FQC   \n",
              "\n",
              "             charttime            storetime value  valuenum valueuom  \\\n",
              "0  2180-03-23 11:51:00  2180-03-23 15:56:00   ___      95.0    mg/dL   \n",
              "1  2180-03-23 11:51:00  2180-03-23 16:00:00   NEG       NaN      NaN   \n",
              "2  2180-03-23 11:51:00  2180-03-23 16:00:00   NEG       NaN      NaN   \n",
              "\n",
              "   ref_range_lower  ref_range_upper  flag priority  \\\n",
              "0             70.0            100.0   NaN  ROUTINE   \n",
              "1              NaN              NaN   NaN  ROUTINE   \n",
              "2              NaN              NaN   NaN  ROUTINE   \n",
              "\n",
              "                                            comments  \n",
              "0  IF FASTING, 70-100 NORMAL, >125 PROVISIONAL DI...  \n",
              "1                                                NaN  \n",
              "2                                                NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 快速预览：列出并显示关键 CSV 的列名和前三行（如果存在）\n",
        "import os, pandas as pd\n",
        "paths = {\n",
        "    'admissions': os.path.join(DATA_DIR, 'hosp', 'admissions.csv'),\n",
        "    'patients': os.path.join(DATA_DIR, 'hosp', 'patients.csv'),\n",
        "    'diagnoses_icd': os.path.join(DATA_DIR, 'hosp', 'diagnoses_icd.csv'),\n",
        "    'chartevents': os.path.join(DATA_DIR, 'icu', 'chartevents.csv'),\n",
        "    'labevents': os.path.join(DATA_DIR, 'hosp', 'labevents.csv'),\n",
        "}\n",
        "from IPython.display import display\n",
        "for name, p in paths.items():\n",
        "    print('---', name, '->', p)\n",
        "    if os.path.exists(p):\n",
        "        try:\n",
        "            df = pd.read_csv(p, nrows=5)\n",
        "            print('columns:', list(df.columns))\n",
        "            print('preview:')\n",
        "            display(df.head(3))\n",
        "        except Exception as e:\n",
        "            print('读取失败:', e)\n",
        "    else:\n",
        "        print('NOT FOUND')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e42f9daa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在计算 30 天再入院标签...\n",
            "总入院次数: 546028\n",
            "30天再入院数: 107560 (19.70%)\n",
            "非再入院数: 438468 (80.30%)\n",
            "\n",
            "前 10 个入院记录的再入院标签:\n",
            "   subject_id   hadm_id           admittime           dischtime  readmit_30d\n",
            "0    10000032  22595853 2180-05-06 22:23:00 2180-05-07 17:15:00            0\n",
            "1    10000032  22841357 2180-06-26 18:27:00 2180-06-27 18:49:00            1\n",
            "2    10000032  29079034 2180-07-23 12:35:00 2180-07-25 17:55:00            1\n",
            "3    10000032  25742920 2180-08-05 23:44:00 2180-08-07 17:50:00            0\n",
            "4    10000068  25022803 2160-03-03 23:16:00 2160-03-04 06:26:00            0\n",
            "5    10000084  23052089 2160-11-21 01:56:00 2160-11-25 14:52:00            0\n",
            "6    10000084  29888819 2160-12-28 05:11:00 2160-12-28 16:07:00            0\n",
            "7    10000108  27250926 2163-09-27 23:17:00 2163-09-28 09:04:00            0\n",
            "8    10000117  22927623 2181-11-15 02:05:00 2181-11-15 14:52:00            0\n",
            "9    10000117  27988844 2183-09-18 18:10:00 2183-09-21 16:30:00            0\n"
          ]
        }
      ],
      "source": [
        "# --------- 计算 30 天再入院标签 ---------\n",
        "# 读取 admissions 数据并解析日期列\n",
        "print('正在计算 30 天再入院标签...')\n",
        "\n",
        "# 辅助函数：尝试解析日期列\n",
        "def _parse_dates_if_exists(df, cols):\n",
        "    \"\"\"如果列存在，则解析为 datetime\"\"\"\n",
        "    for col in cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "    return df\n",
        "\n",
        "# 读取 admissions 数据\n",
        "admissions_path = os.path.join(DATA_DIR, 'hosp', 'admissions.csv')\n",
        "admissions = pd.read_csv(admissions_path)\n",
        "admissions = _parse_dates_if_exists(admissions, ['admittime', 'dischtime', 'deathtime', 'edregtime', 'edouttime'])\n",
        "\n",
        "# 按 subject_id 和 admittime 排序\n",
        "admissions = admissions.sort_values(['subject_id', 'admittime']).reset_index(drop=True)\n",
        "\n",
        "# 为每个入院记录计算是否有 30 天内的再入院\n",
        "readmit_30d = []\n",
        "for idx, row in admissions.iterrows():\n",
        "    subject_id = row['subject_id']\n",
        "    hadm_id = row['hadm_id']\n",
        "    dischtime = row['dischtime']\n",
        "    \n",
        "    # 如果没有出院时间，跳过（无法计算再入院）\n",
        "    if pd.isna(dischtime):\n",
        "        readmit_30d.append(0)\n",
        "        continue\n",
        "    \n",
        "    # 查找同一患者的后续入院记录\n",
        "    future_admissions = admissions[\n",
        "        (admissions['subject_id'] == subject_id) & \n",
        "        (admissions['hadm_id'] != hadm_id) &  # 排除当前入院\n",
        "        (admissions['admittime'] > dischtime)  # 必须在出院后\n",
        "    ]\n",
        "    \n",
        "    # 检查是否有在 30 天内的再入院\n",
        "    if not future_admissions.empty:\n",
        "        # 找到最早的后续入院\n",
        "        next_admit = future_admissions.iloc[0]['admittime']\n",
        "        days_to_readmit = (next_admit - dischtime).total_seconds() / (24 * 3600)\n",
        "        \n",
        "        if days_to_readmit <= 30:\n",
        "            readmit_30d.append(1)\n",
        "        else:\n",
        "            readmit_30d.append(0)\n",
        "    else:\n",
        "        readmit_30d.append(0)\n",
        "\n",
        "# 将标签添加到 admissions 数据框\n",
        "admissions['readmit_30d'] = readmit_30d\n",
        "\n",
        "# 显示标签分布\n",
        "print(f'总入院次数: {len(admissions)}')\n",
        "print(f'30天再入院数: {sum(readmit_30d)} ({100*sum(readmit_30d)/len(readmit_30d):.2f}%)')\n",
        "print(f'非再入院数: {len(readmit_30d) - sum(readmit_30d)} ({100*(len(readmit_30d)-sum(readmit_30d))/len(readmit_30d):.2f}%)')\n",
        "print('\\n前 10 个入院记录的再入院标签:')\n",
        "print(admissions[['subject_id', 'hadm_id', 'admittime', 'dischtime', 'readmit_30d']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 主代码（较长）\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 深度学习依赖 (PyTorch)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ARIMA\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# SHAP\n",
        "import shap\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a79cb2cb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x16955d5f0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --------- 设置 ---------\n",
        "# 使用上面第一个 cell 中设置的 DATA_DIR。如果未定义，请在本 cell 顶部手动修改。\n",
        "try:\n",
        "    DATA_DIR  # noqa: F821\n",
        "except NameError:\n",
        "    DATA_DIR = '/Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1'\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b8bcb3a1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "加载 CSV（示例）...\n"
          ]
        }
      ],
      "source": [
        "# --------- 读数据（示例） ---------\n",
        "print('加载 CSV（示例）...')\n",
        "# 尝试从 hosp/ 和 icu/ 子目录读取文件，优先使用 hosp/ 下的行政数据和 icu/ 下的监测数据。\n",
        "hosp_dir = os.path.join(DATA_DIR, 'hosp')\n",
        "icu_dir = os.path.join(DATA_DIR, 'icu')\n",
        "def _read(path, **kwargs):\n",
        "    if os.path.exists(path):\n",
        "        return pd.read_csv(path, **kwargs)\n",
        "    else:\n",
        "        raise FileNotFoundError(f'未找到文件: {path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "846a1a8f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在加载时序数据（可能需要几分钟）...\n"
          ]
        },
        {
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 11 fields in line 163446604, saw 21\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 读取时序数据（chartevents 和 labevents）\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 注意：这些文件可能很大，实际使用时考虑分块读取或筛选\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m正在加载时序数据（可能需要几分钟）...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m chartevents = \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43micu_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchartevents.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m chartevents = _parse_dates_if_exists(chartevents, [\u001b[33m'\u001b[39m\u001b[33mcharttime\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstoretime\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     11\u001b[39m labevents = _read(os.path.join(hosp_dir, \u001b[33m'\u001b[39m\u001b[33mlabevents.csv\u001b[39m\u001b[33m'\u001b[39m))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(path, **kwargs)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read\u001b[39m(path, **kwargs):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(path):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m未找到文件: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mimic-py311/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mimic-py311/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mimic-py311/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mimic-py311/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 11 fields in line 163446604, saw 21\n"
          ]
        }
      ],
      "source": [
        "# --------- 读取患者和诊断数据 ---------\n",
        "# 注意：admissions 数据已在上一个 cell 中加载并包含 readmit_30d 标签\n",
        "patients = _read(os.path.join(hosp_dir, 'patients.csv'))\n",
        "diagnoses = _read(os.path.join(hosp_dir, 'diagnoses_icd.csv'))\n",
        "\n",
        "# 读取时序数据（chartevents 和 labevents）\n",
        "# 注意：这些文件可能很大，实际使用时考虑分块读取或筛选\n",
        "print('正在加载时序数据（可能需要几分钟）...')\n",
        "chartevents = _read(os.path.join(icu_dir, 'chartevents.csv'))\n",
        "chartevents = _parse_dates_if_exists(chartevents, ['charttime', 'storetime'])\n",
        "labevents = _read(os.path.join(hosp_dir, 'labevents.csv'))\n",
        "labevents = _parse_dates_if_exists(labevents, ['charttime', 'storetime'])\n",
        "print(f'chartevents: {len(chartevents)} 行')\n",
        "print(f'labevents: {len(labevents)} 行')\n",
        "\n",
        "# --------- 定义慢性病筛选 ICD 列表（示例：糖尿病、心衰、COPD、CKD） ---------\n",
        "chronic_icd_list = {\n",
        "    'diabetes': ['250.00', '250.01', 'E11'],\n",
        "    'heart_failure': ['428.0','I50'],\n",
        "    'copd': ['496','J44'],\n",
        "    'ckd': ['585','N18']\n",
        "}\n",
        "\n",
        "def patient_has_chronic(subject_id):\n",
        "    rows = diagnoses[diagnoses['subject_id']==subject_id]\n",
        "    if rows.empty:\n",
        "        return False\n",
        "    icd_codes = rows['icd_code'].astype(str).str.upper().tolist()\n",
        "    for name, code_list in chronic_icd_list.items():\n",
        "        for c in code_list:\n",
        "            for icd in icd_codes:\n",
        "                if c in icd:\n",
        "                    return True\n",
        "    return False\n",
        "\n",
        "# 抽样筛选（示例）\n",
        "unique_subjects = admissions['subject_id'].unique()[:5000]\n",
        "selected_subjects = [s for s in unique_subjects if patient_has_chronic(s)]\n",
        "print(f'筛到慢性病患者: {len(selected_subjects)} (示例抽样)')\n",
        "\n",
        "# 静态特征\n",
        "adm = admissions[admissions['subject_id'].isin(selected_subjects)].copy()\n",
        "pat = patients[patients['subject_id'].isin(selected_subjects)].copy()\n",
        "df = adm.merge(pat, on='subject_id', how='left')\n",
        "\n",
        "# 计算年龄：MIMIC-IV 使用 anchor_age 而不是出生日期\n",
        "# anchor_age 是患者在 anchor_year 时的年龄\n",
        "# 计算入院时的年龄：anchor_age + (admittime 年份 - anchor_year)\n",
        "df['admit_year'] = df['admittime'].dt.year\n",
        "df['age'] = df['anchor_age'] + (df['admit_year'] - df['anchor_year'])\n",
        "df['age'] = df['age'].clip(lower=0)  # 确保年龄非负\n",
        "\n",
        "df['gender'] = df['gender'].map({'M':1,'F':0}).fillna(0).astype(int)\n",
        "df['los_days'] = (df['dischtime'] - df['admittime']).dt.total_seconds()/(3600*24)\n",
        "\n",
        "# 使用 readmit_30d 标签而不是 dead_in_hospital\n",
        "static_features = ['subject_id', 'hadm_id', 'age', 'gender', 'los_days', 'readmit_30d']\n",
        "static_df = df[static_features].drop_duplicates().reset_index(drop=True)\n",
        "print(static_df.head())\n",
        "print(f'\\n标签分布 - readmit_30d: {static_df[\"readmit_30d\"].value_counts().to_dict()}')\n",
        "\n",
        "# 时序变量的 ITEMID 示例（请用真实 ITEMID 表替换）\n",
        "vital_items_of_interest = {\n",
        "    'heart_rate': [211, 220045],\n",
        "    'sys_bp': [220179, 51],\n",
        "    'dias_bp': [220180, 8368],\n",
        "    'resp_rate': [220210, 618],\n",
        "    'temp': [223761, 678],\n",
        "    'spo2': [220277]\n",
        "}\n",
        "lab_items_of_interest = {\n",
        "    'creatinine': [50912],\n",
        "    'glucose': [807, 823],\n",
        "    'wbc': [730],\n",
        "}\n",
        "\n",
        "def get_patient_events(hadm_id, window_hours=72, resample_freq='1H'):\n",
        "    adm_row = adm[adm['hadm_id']==hadm_id].iloc[0]\n",
        "    t0 = adm_row['admittime']\n",
        "    t_end = t0 + pd.Timedelta(hours=window_hours)\n",
        "    ce = chartevents[(chartevents['hadm_id']==hadm_id) & (chartevents['charttime']>=t0) & (chartevents['charttime']<=t_end)]\n",
        "    le = labevents[(labevents['hadm_id']==hadm_id) & (labevents['charttime']>=t0) & (labevents['charttime']<=t_end)]\n",
        "    rows = []\n",
        "    for var, itemids in vital_items_of_interest.items():\n",
        "        tmp = ce[ce['itemid'].isin(itemids)][['charttime','value']].copy()\n",
        "        if tmp.empty:\n",
        "            continue\n",
        "        tmp = tmp.rename(columns={'charttime':'time','value':var})\n",
        "        rows.append(tmp.set_index('time')[var])\n",
        "    for var, itemids in lab_items_of_interest.items():\n",
        "        tmp = le[le['itemid'].isin(itemids)][['charttime','value']].copy()\n",
        "        if tmp.empty:\n",
        "            continue\n",
        "        tmp = tmp.rename(columns={'charttime':'time','value':var})\n",
        "        rows.append(tmp.set_index('time')[var])\n",
        "    if not rows:\n",
        "        return None\n",
        "    combined = pd.concat(rows, axis=1)\n",
        "    combined = combined.resample(resample_freq).mean()\n",
        "    combined = combined[:t0 + pd.Timedelta(hours=window_hours)]\n",
        "    return combined\n",
        "\n",
        "MAX_HOURS = 72\n",
        "RESAMPLE_FREQ = '1H'\n",
        "TIMESTEPS = int(MAX_HOURS)\n",
        "\n",
        "def build_dataset(hadm_ids):\n",
        "    X_static = []\n",
        "    X_ts = []\n",
        "    y = []\n",
        "    for hid in hadm_ids:\n",
        "        s = static_df[static_df['hadm_id']==hid]\n",
        "        if s.empty:\n",
        "            continue\n",
        "        ts = get_patient_events(hid, window_hours=MAX_HOURS, resample_freq=RESAMPLE_FREQ)\n",
        "        if ts is None:\n",
        "            continue\n",
        "        all_cols = list(vital_items_of_interest.keys()) + list(lab_items_of_interest.keys())\n",
        "        for c in all_cols:\n",
        "            if c not in ts.columns:\n",
        "                ts[c] = np.nan\n",
        "        ts = ts[all_cols]\n",
        "        if len(ts) < TIMESTEPS:\n",
        "            pad_len = TIMESTEPS - len(ts)\n",
        "            pad_df = pd.DataFrame(np.nan, index=pd.date_range(ts.index[-1]+pd.Timedelta(hours=1), periods=pad_len, freq=RESAMPLE_FREQ), columns=ts.columns)\n",
        "            ts = pd.concat([ts, pad_df])\n",
        "        else:\n",
        "            ts = ts.iloc[:TIMESTEPS]\n",
        "        X_static.append(s[['age','gender','los_days']].iloc[0].values.astype(float))\n",
        "        X_ts.append(ts.values.astype(float))\n",
        "        y.append(int(s['readmit_30d'].iloc[0]))  # 使用 30 天再入院标签\n",
        "    X_static = np.array(X_static)\n",
        "    X_ts = np.array(X_ts)\n",
        "    y = np.array(y)\n",
        "    return X_static, X_ts, y\n",
        "\n",
        "hadm_ids = static_df['hadm_id'].unique()[:500]\n",
        "X_static, X_ts, y = build_dataset(hadm_ids)\n",
        "print('Shape:', X_static.shape, X_ts.shape, y.shape)\n",
        "\n",
        "def impute_time_series_array(X_ts):\n",
        "    N, T, F = X_ts.shape\n",
        "    X_imputed = X_ts.copy()\n",
        "    for i in range(N):\n",
        "        df_ts = pd.DataFrame(X_ts[i], columns=[f'f{j}' for j in range(F)])\n",
        "        df_ts = df_ts.interpolate(method='linear', limit_direction='both', axis=0).ffill().bfill()\n",
        "        df_ts = df_ts.fillna(df_ts.mean())\n",
        "        X_imputed[i] = df_ts.values\n",
        "    return X_imputed\n",
        "\n",
        "X_ts_imputed = impute_time_series_array(X_ts)\n",
        "scaler = StandardScaler()\n",
        "X_static_scaled = scaler.fit_transform(X_static)\n",
        "\n",
        "def pool_time_series_features(X_ts):\n",
        "    N, T, F = X_ts.shape\n",
        "    feats = []\n",
        "    for i in range(N):\n",
        "        arr = X_ts[i]\n",
        "        mean = np.nanmean(arr, axis=0)\n",
        "        std = np.nanstd(arr, axis=0)\n",
        "        mn = np.nanmin(arr, axis=0)\n",
        "        mx = np.nanmax(arr, axis=0)\n",
        "        feats.append(np.concatenate([mean,std,mn,mx]))\n",
        "    return np.array(feats)\n",
        "\n",
        "X_pool = pool_time_series_features(X_ts_imputed)\n",
        "X_final = np.hstack([X_static_scaled, X_pool])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_proba = lr.predict_proba(X_test)[:,1]\n",
        "print('LogReg AUC:', roc_auc_score(y_test, y_pred_proba))\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_SEED, n_jobs=4)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_proba_rf = rf.predict_proba(X_test)[:,1]\n",
        "print('RandomForest AUC:', roc_auc_score(y_test, y_pred_proba_rf))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f6bd2549",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始模型训练与评估（分层 CV）...\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "请先运行前面的数据构造单元以生成 X_final 和 y",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 使用 X_final, y 已由上文构造\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mX_final\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m(), \u001b[33m'\u001b[39m\u001b[33m请先运行前面的数据构造单元以生成 X_final 和 y\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 简单的 5 折分层交叉验证评估（LogReg + RandomForest）\u001b[39;00m\n\u001b[32m     11\u001b[39m skf = StratifiedKFold(n_splits=\u001b[32m5\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=RANDOM_SEED)\n",
            "\u001b[31mAssertionError\u001b[39m: 请先运行前面的数据构造单元以生成 X_final 和 y"
          ]
        }
      ],
      "source": [
        "# --------- 模型训练与评估（分层 CV） ---------\n",
        "print('开始模型训练与评估（分层 CV）...')\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
        "import joblib\n",
        "\n",
        "# 使用 X_final, y 已由上文构造\n",
        "assert 'X_final' in globals() and 'y' in globals(), '请先运行前面的数据构造单元以生成 X_final 和 y'\n",
        "\n",
        "# 简单的 5 折分层交叉验证评估（LogReg + RandomForest）\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "models = {\n",
        "    'logreg': LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
        "    'rf': RandomForestClassifier(n_estimators=200, random_state=RANDOM_SEED, n_jobs=4, class_weight='balanced')\n",
        "}\n",
        "cv_results = {m: {'auroc': [], 'auprc': []} for m in models}\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_final, y)):\n",
        "    print(f'Fold {fold+1}/5')\n",
        "    Xtr, Xval = X_final[train_idx], X_final[val_idx]\n",
        "    ytr, yval = y[train_idx], y[val_idx]\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.fit(Xtr, ytr)\n",
        "        probs = model.predict_proba(Xval)[:,1]\n",
        "        auroc = roc_auc_score(yval, probs) if len(set(yval))>1 else 0.5\n",
        "        auprc = average_precision_score(yval, probs) if len(set(yval))>1 else 0.0\n",
        "        cv_results[name]['auroc'].append(auroc)\n",
        "        cv_results[name]['auprc'].append(auprc)\n",
        "        print(f'  {name} AUROC={auroc:.4f} AUPRC={auprc:.4f}')\n",
        "\n",
        "print('\\nCV Summary:')\n",
        "for name in models:\n",
        "    print(f\"{name}: AUROC mean={np.mean(cv_results[name]['auroc']):.4f} std={np.std(cv_results[name]['auroc']):.4f} | AUPRC mean={np.mean(cv_results[name]['auprc']):.4f} std={np.std(cv_results[name]['auprc']):.4f}\")\n",
        "\n",
        "# 训练最终模型在全部训练数据上并保存\n",
        "print('\\n训练最终模型并保存...')\n",
        "for name, model in models.items():\n",
        "    model.fit(X_final, y)\n",
        "    joblib.dump(model, f'{name}_readmit30_model.pkl')\n",
        "    print(f'Saved {name}_readmit30_model.pkl')\n",
        "\n",
        "# 保存 scaler（如果存在）\n",
        "if 'scaler' in globals():\n",
        "    joblib.dump(scaler, 'static_scaler.pkl')\n",
        "    print('Saved static_scaler.pkl')\n",
        "\n",
        "print('训练与评估完成。')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3878840",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ARIMA 示例略（请在单变量非平稳/平稳检测后运行）\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X_static, X_ts, y):\n",
        "        self.X_static = torch.tensor(X_static, dtype=torch.float32)\n",
        "        self.X_ts = torch.tensor(X_ts, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X_static[idx], self.X_ts[idx], self.y[idx]\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=1, static_size=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size + static_size, 1)\n",
        "    def forward(self, x_static, x_ts):\n",
        "        out, (hn, cn) = self.lstm(x_ts)\n",
        "        h_last = out[:, -1, :]\n",
        "        feats = torch.cat([h_last, x_static], dim=1)\n",
        "        feats = self.dropout(feats)\n",
        "        logits = self.fc(feats)\n",
        "        return torch.sigmoid(logits).squeeze(1)\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, input_size, d_model=64, nhead=4, num_layers=2, static_size=3, dim_feedforward=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_size, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(d_model + static_size, 1)\n",
        "    def forward(self, x_static, x_ts):\n",
        "        x = self.input_proj(x_ts)\n",
        "        x = self.transformer(x)\n",
        "        x_pooled = x.mean(dim=1)\n",
        "        feats = torch.cat([x_pooled, x_static], dim=1)\n",
        "        logits = self.fc(feats)\n",
        "        return torch.sigmoid(logits).squeeze(1)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=10, lr=1e-3, device='cpu'):\n",
        "    model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.BCELoss()\n",
        "    best_auc = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for xs, xt, yb in train_loader:\n",
        "            xs = xs.to(device); xt = xt.to(device); yb = yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            ypred = model(xs, xt)\n",
        "            loss = loss_fn(ypred, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_losses.append(loss.item())\n",
        "        model.eval()\n",
        "        ys, preds = [], []\n",
        "        with torch.no_grad():\n",
        "            for xs, xt, yb in val_loader:\n",
        "                xs = xs.to(device); xt = xt.to(device)\n",
        "                p = model(xs, xt).cpu().numpy()\n",
        "                preds.extend(p.tolist())\n",
        "                ys.extend(yb.numpy().tolist())\n",
        "        auc = roc_auc_score(ys, preds) if len(set(ys))>1 else 0.5\n",
        "        print(f'Epoch {epoch+1}/{epochs} train_loss={np.mean(train_losses):.4f} val_auc={auc:.4f}')\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "    return best_auc\n",
        "\n",
        "Xts = X_ts_imputed\n",
        "Xst = X_static_scaled[:Xts.shape[0]]\n",
        "y_small = y[:Xts.shape[0]]\n",
        "Xtr_s, Xval_s, Xtr_ts, Xval_ts, ytr, yval = train_test_split(Xst, Xts, y_small, test_size=0.2, stratify=y_small, random_state=RANDOM_SEED)\n",
        "train_ds = TimeSeriesDataset(Xtr_s, Xtr_ts, ytr)\n",
        "val_ds = TimeSeriesDataset(Xval_s, Xval_ts, yval)\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=32)\n",
        "\n",
        "input_size = Xts.shape[2]\n",
        "lstm_model = LSTMClassifier(input_size=input_size, hidden_size=64, static_size=Xst.shape[1])\n",
        "print('训练 LSTM 模型（示例）...')\n",
        "train_model(lstm_model, train_loader, val_loader, epochs=5, lr=1e-3, device='cpu')\n",
        "\n",
        "transformer_model = TransformerClassifier(input_size=input_size, d_model=64, static_size=Xst.shape[1])\n",
        "print('训练 Transformer 模型（示例）...')\n",
        "train_model(transformer_model, train_loader, val_loader, epochs=5, lr=1e-3, device='cpu')\n",
        "\n",
        "print('用 SHAP 分析 RandomForest 特征重要性（示例）')\n",
        "explainer = shap.TreeExplainer(rf)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "shap.summary_plot(shap_values[1], X_test, feature_names=None, show=False)\n",
        "plt.title('SHAP Summary: RandomForest')\n",
        "plt.savefig('shap_summary_rf.png')\n",
        "plt.close()\n",
        "\n",
        "import joblib\n",
        "joblib.dump(scaler, 'static_scaler.pkl')\n",
        "joblib.dump(rf, 'rf_model.pkl')\n",
        "joblib.dump(lr, 'lr_model.pkl')\n",
        "print('完成。模型与图像已保存到当前目录。')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mimic-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
