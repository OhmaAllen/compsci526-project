{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ef0b69",
   "metadata": {},
   "source": [
    "# MIMIC-IV 30-Day Readmission Prediction - Baseline Models\n",
    "## Comprehensive Classification Model Comparison\n",
    "\n",
    "**Objective**: Build and evaluate baseline machine learning models for 30-day hospital readmission\n",
    "\n",
    "**Models**: Logistic Regression, Random Forest, XGBoost, SVM, Naive Bayes\n",
    "\n",
    "**Metrics**: AUROC, AUPRC, F1-Score, Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21f484",
   "metadata": {},
   "source": [
    "## Section 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a45acd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, auc, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, classification_report, f1_score,\n",
    "    accuracy_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60260d",
   "metadata": {},
   "source": [
    "## Section 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe4ae6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (546038, 68)\n",
      "Target distribution:\n",
      "readmit_30d\n",
      "0    438774\n",
      "1    107264\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/yuchenzhou/Documents/duke/compsci526/final_proj/mimic_data/processed_data'\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, 'readmission_features_cleaned.csv'))\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Target distribution:\\n{df['readmit_30d'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2156eb86",
   "metadata": {},
   "source": [
    "## Section 3: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e83612b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 64\n",
      "Target - 0: 438774, 1: 107264\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = [col for col in df.columns if col in ['subject_id', 'hadm_id', 'primary_icd_code']]\n",
    "df_processed = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "X = df_processed.drop(columns=['readmit_30d'])\n",
    "y = df_processed['readmit_30d'].copy()\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Target - 0: {(y==0).sum()}, 1: {(y==1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edcb8e8",
   "metadata": {},
   "source": [
    "## Section 4: Train-Test Split & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fabf8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (436830, 64), Test: (109208, 64)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print(f\"Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035f468",
   "metadata": {},
   "source": [
    "## Section 5-7: Build & Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac449b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression... âœ…\n",
      "Training Random Forest... âœ…\n",
      "Training XGBoost... âœ…\n",
      "Training SVM... "
     ]
    }
   ],
   "source": [
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, scale_pos_weight=scale_pos_weight, eval_metric='logloss'),\n",
    "    'SVM': SVC(kernel='rbf', C=1.0, random_state=42, probability=True, class_weight='balanced'),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\", end=\" \")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    y_test_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    results[name] = {\n",
    "        'y_test_pred': y_test_pred,\n",
    "        'y_test_pred_proba': y_test_pred_proba,\n",
    "        'model': model\n",
    "    }\n",
    "    print(\"âœ…\")\n",
    "\n",
    "print(\"\\nâœ… All models trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c003b9",
   "metadata": {},
   "source": [
    "## Section 8: Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bd671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summary = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"{'Model':<20} {'AUROC':<12} {'AUPRC':<12} {'F1':<12} {'Accuracy':<12} {'Precision':<12} {'Recall':<12}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for name, result in results.items():\n",
    "    y_pred = result['y_test_pred']\n",
    "    y_pred_proba = result['y_test_pred_proba']\n",
    "    \n",
    "    auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    auprc = auc(recall_curve, precision_curve)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    metrics_summary[name] = {\n",
    "        'AUROC': auroc,\n",
    "        'AUPRC': auprc,\n",
    "        'F1': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'y_test_pred': y_pred,\n",
    "        'y_test_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:<20} {auroc:<12.4f} {auprc:<12.4f} {f1:<12.4f} {accuracy:<12.4f} {precision:<12.4f} {recall:<12.4f}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "best_model = max(metrics_summary, key=lambda x: metrics_summary[x]['AUROC'])\n",
    "print(f\"\\nğŸ† Best Model: {best_model} (AUROC: {metrics_summary[best_model]['AUROC']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a9b1d",
   "metadata": {},
   "source": [
    "## Section 9: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e010b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "for name, metrics in metrics_summary.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, metrics['y_test_pred_proba'])\n",
    "    ax.plot(fpr, tpr, label=f\"{name} (AUROC={metrics['AUROC']:.4f})\", linewidth=2)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# AUROC Comparison\n",
    "ax = axes[1]\n",
    "names = list(metrics_summary.keys())\n",
    "auroc_scores = [metrics_summary[name]['AUROC'] for name in names]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(names)))\n",
    "\n",
    "bars = ax.barh(names, auroc_scores, color=colors)\n",
    "ax.set_xlabel('AUROC')\n",
    "ax.set_xlim([0.5, 1.0])\n",
    "ax.set_title('Model Comparison')\n",
    "\n",
    "for i, (bar, score) in enumerate(zip(bars, auroc_scores)):\n",
    "    ax.text(score + 0.01, i, f'{score:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(data_dir, 'roc_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… ROC plots saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7652e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, metrics) in enumerate(sorted(metrics_summary.items())):\n",
    "    cm = confusion_matrix(y_test, metrics['y_test_pred'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], cbar=False,\n",
    "                xticklabels=['No', 'Yes'],\n",
    "                yticklabels=['No', 'Yes'])\n",
    "    axes[idx].set_title(f'{name} (F1={metrics[\"F1\"]:.4f})')\n",
    "    axes[idx].set_ylabel('True')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(data_dir, 'confusion_matrices.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Confusion matrices saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics Heatmap\n",
    "metrics_df = pd.DataFrame(metrics_summary).T\n",
    "metrics_cols = ['AUROC', 'AUPRC', 'F1', 'Accuracy', 'Precision', 'Recall']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.heatmap(metrics_df[metrics_cols], annot=True, fmt='.4f', cmap='RdYlGn', ax=ax,\n",
    "            cbar_kws={'label': 'Score'}, vmin=0, vmax=1, linewidths=1)\n",
    "\n",
    "ax.set_title('Model Performance Metrics')\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Models')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(data_dir, 'metrics_heatmap.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Metrics heatmap saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e725f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest\n",
    "rf_model = results['Random Forest']['model']\n",
    "fi_rf = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "axes[0].barh(range(len(fi_rf)), fi_rf['importance'], color='steelblue')\n",
    "axes[0].set_yticks(range(len(fi_rf)))\n",
    "axes[0].set_yticklabels(fi_rf['feature'])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Random Forest - Top 20')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = results['Gradient Boosting']['model']\n",
    "fi_gb = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': gb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "axes[1].barh(range(len(fi_gb)), fi_gb['importance'], color='coral')\n",
    "axes[1].set_yticks(range(len(fi_gb)))\n",
    "axes[1].set_yticklabels(fi_gb['feature'])\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Gradient Boosting - Top 20')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(data_dir, 'feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Feature importance saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3e2a7",
   "metadata": {},
   "source": [
    "## Section 10: Cross-Validation & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2747815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Cross-Validation (5-Fold)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "    cv_results[name] = {'mean': cv_scores.mean(), 'std': cv_scores.std()}\n",
    "    print(f\"{name:<20} AUROC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "# Save results\n",
    "report = f\"\"\"\n",
    "{'='*100}\n",
    "MIMIC-IV 30-Day Readmission - Baseline Models Report\n",
    "{'='*100}\n",
    "\n",
    "ã€Test Set Performanceã€‘\n",
    "{'-'*100}\n",
    "{'Model':<20} {'AUROC':<12} {'AUPRC':<12} {'F1':<12} {'Accuracy':<12}\n",
    "{'-'*100}\n",
    "\"\"\"\n",
    "\n",
    "for name in sorted(metrics_summary.keys()):\n",
    "    m = metrics_summary[name]\n",
    "    report += f\"{name:<20} {m['AUROC']:<12.4f} {m['AUPRC']:<12.4f} {m['F1']:<12.4f} {m['Accuracy']:<12.4f}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "ã€Cross-Validation Resultsã€‘\n",
    "{'-'*100}\n",
    "{'Model':<20} {'CV AUROC Mean':<18} {'Std Dev':<12}\n",
    "{'-'*100}\n",
    "\"\"\"\n",
    "\n",
    "for name in sorted(cv_results.keys()):\n",
    "    report += f\"{name:<20} {cv_results[name]['mean']:<18.4f} {cv_results[name]['std']:<12.4f}\\n\"\n",
    "\n",
    "best_model = max(metrics_summary, key=lambda x: metrics_summary[x]['AUROC'])\n",
    "report += f\"\"\"\n",
    "ã€Best Modelã€‘\n",
    "{'-'*100}\n",
    "Model: {best_model}\n",
    "Test AUROC: {metrics_summary[best_model]['AUROC']:.4f}\n",
    "CV AUROC: {cv_results[best_model]['mean']:.4f} (+/- {cv_results[best_model]['std']:.4f})\n",
    "\n",
    "{'='*100}\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "report_path = os.path.join(data_dir, 'baseline_models_report.txt')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Save metrics to JSON\n",
    "metrics_json = {}\n",
    "for name, m in metrics_summary.items():\n",
    "    metrics_json[name] = {\n",
    "        'AUROC': float(m['AUROC']),\n",
    "        'AUPRC': float(m['AUPRC']),\n",
    "        'F1': float(m['F1']),\n",
    "        'Accuracy': float(m['Accuracy']),\n",
    "        'Precision': float(m['Precision']),\n",
    "        'Recall': float(m['Recall']),\n",
    "        'CV_Mean': float(cv_results[name]['mean']),\n",
    "        'CV_Std': float(cv_results[name]['std'])\n",
    "    }\n",
    "\n",
    "json_path = os.path.join(data_dir, 'baseline_models_metrics.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(metrics_json, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Results saved to {data_dir}\")\n",
    "print(f\"   - Report: baseline_models_report.txt\")\n",
    "print(f\"   - Metrics: baseline_models_metrics.json\")\n",
    "print(f\"   - Plots: roc_comparison.png, confusion_matrices.png, metrics_heatmap.png, feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267d6ce",
   "metadata": {},
   "source": [
    "# MIMIC-IV 30-Day Readmission Prediction - Baseline Models\n",
    "## Comprehensive Classification Model Comparison\n",
    "\n",
    "**Objective**: Build and evaluate baseline machine learning models for predicting 30-day hospital readmission\n",
    "\n",
    "**Models to Compare**:\n",
    "1. Logistic Regression (Linear Baseline)\n",
    "2. Random Forest (Tree-based Baseline)\n",
    "3. Gradient Boosting (Gradient Boosting)\n",
    "4. SVM (Support Vector Machine)\n",
    "5. Naive Bayes (Probabilistic Baseline)\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- AUROC (Area Under Receiver Operating Characteristic Curve)\n",
    "- AUPRC (Area Under Precision-Recall Curve)\n",
    "- F1-Score\n",
    "- Accuracy\n",
    "- Precision & Recall\n",
    "\n",
    "**Generated**: 2025-10-16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ed651",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d40bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/yuchenzhou/Documents/duke/compsci526/final_proj/git_proj/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <E8D72161-CCD1-3423-9388-36D4CA0A7524> /Users/yuchenzhou/Documents/duke/compsci526/final_proj/git_proj/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libomp.dylib' (no such file), '/opt/homebrew/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnaive_bayes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GaussianNB\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Import XGBoost\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgb\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Import visualization libraries\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/duke/compsci526/final_proj/git_proj/.venv/lib/python3.13/site-packages/xgboost/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     Booster,\n\u001b[32m     10\u001b[39m     DataIter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     build_info,\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/duke/compsci526/final_proj/git_proj/.venv/lib/python3.13/site-packages/xgboost/tracker.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, _deprecate_positional_args, make_jcargs\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/duke/compsci526/final_proj/git_proj/.venv/lib/python3.13/site-packages/xgboost/core.py:295\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m _LIB = \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \u001b[33;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m \u001b[33;03m        return value from API calls\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/duke/compsci526/final_proj/git_proj/.venv/lib/python3.13/site-packages/xgboost/core.py:257\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[32m    256\u001b[39m         libname = os.path.basename(lib_paths[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[32m    258\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[33mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) could not be loaded.\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[33mLikely causes:\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[33m  * OpenMP runtime is not installed\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[33m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[33m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[33m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[33m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[32m    266\u001b[39m \n\u001b[32m    267\u001b[39m \u001b[33m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[32m    268\u001b[39m \n\u001b[32m    269\u001b[39m \u001b[33mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    271\u001b[39m         )\n\u001b[32m    272\u001b[39m     _register_log_callback(lib)\n\u001b[32m    274\u001b[39m     libver = _lib_version(lib)\n",
      "\u001b[31mXGBoostError\u001b[39m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/yuchenzhou/Documents/duke/compsci526/final_proj/git_proj/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <E8D72161-CCD1-3423-9388-36D4CA0A7524> /Users/yuchenzhou/Documents/duke/compsci526/final_proj/git_proj/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libomp.dylib' (no such file), '/opt/homebrew/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pandas numpy scikit-learn matplotlib seaborn\n",
    "\n",
    "# Import core data handling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import sklearn utilities\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, auc, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, classification_report, f1_score,\n",
    "    accuracy_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Import baseline models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")\n",
    "print(f\"   scikit-learn version: {__import__('sklearn').__version__}\")\n",
    "print(f\"   Pandas version: {pd.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ef1d1",
   "metadata": {},
   "source": [
    "## Section 2: Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path\n",
    "data_dir = '/Users/yuchenzhou/Documents/duke/compsci526/final_proj/mimic_data/processed_data'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ã€Section 2ã€‘æ•°æ®åŠ è½½å’Œæ¢ç´¢\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250abe76",
   "metadata": {},
   "source": [
    "## Section 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ã€Section 3ã€‘æ•°æ®é¢„å¤„ç†\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1ï¸âƒ£ Remove unnecessary columns\n",
    "print(\"\\n1ï¸âƒ£ ç§»é™¤ä¸å¿…è¦çš„åˆ—...\")\n",
    "cols_to_drop = [col for col in df.columns if col in ['subject_id', 'hadm_id', 'primary_icd_code']]\n",
    "df_processed = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "print(f\"   ç§»é™¤äº† {len(cols_to_drop)} åˆ—\")\n",
    "print(f\"   æ–°æ•°æ®å½¢çŠ¶: {df_processed.shape}\")\n",
    "\n",
    "# 2ï¸âƒ£ Separate features and target\n",
    "print(\"\\n2ï¸âƒ£ åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡...\")\n",
    "X = df_processed.drop(columns=['readmit_30d'])\n",
    "y = df_processed['readmit_30d'].copy()\n",
    "\n",
    "print(f\"   ç‰¹å¾æ•°: {X.shape[1]}\")\n",
    "print(f\"   ç›®æ ‡å˜é‡åˆ†å¸ƒ:\")\n",
    "print(f\"      - 0 (æ— å†å…¥é™¢): {(y==0).sum():,}\")\n",
    "print(f\"      - 1 (30å¤©å†å…¥é™¢): {(y==1).sum():,}\")\n",
    "print(f\"      - ç±»åˆ«ä¸å¹³è¡¡æ¯”: {(y==1).sum() / (y==0).sum():.4f}\")\n",
    "\n",
    "# 3ï¸âƒ£ Check and handle missing values\n",
    "print(\"\\n3ï¸âƒ£ å¤„ç†ç¼ºå¤±å€¼...\")\n",
    "missing_per_col = X.isnull().sum()\n",
    "missing_cols = missing_per_col[missing_per_col > 0]\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    for col, count in missing_cols.items():\n",
    "        X[col].fillna(X[col].median(), inplace=True)\n",
    "    print(f\"   å·²å¤„ç† {len(missing_cols)} åˆ—çš„ç¼ºå¤±å€¼\")\n",
    "else:\n",
    "    print(f\"   âœ… æ— ç¼ºå¤±å€¼\")\n",
    "\n",
    "# 4ï¸âƒ£ Remove duplicates\n",
    "print(\"\\n4ï¸âƒ£ æ£€æŸ¥é‡å¤å€¼...\")\n",
    "duplicates = X.duplicated().sum()\n",
    "print(f\"   é‡å¤è¡Œæ•°: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    X = X.drop_duplicates()\n",
    "    y = y[X.index]\n",
    "    print(f\"   å·²ç§»é™¤é‡å¤è¡Œï¼Œæ–°å½¢çŠ¶: {X.shape}\")\n",
    "\n",
    "print(\"\\nâœ… æ•°æ®é¢„å¤„ç†å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe04aad0",
   "metadata": {},
   "source": [
    "## Section 4: Split Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ã€Section 4ã€‘æ•°æ®é›†åˆ’åˆ† (è®­ç»ƒ/æµ‹è¯•)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Split data: 80% train, 20% test\n",
    "print(\"\\n1ï¸âƒ£ ä½¿ç”¨ stratified split åˆ’åˆ†æ•°æ® (80/20)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"   âœ… åˆ’åˆ†å®Œæˆ\")\n",
    "print(f\"   è®­ç»ƒé›†: {X_train.shape[0]:,} æ ·æœ¬\")\n",
    "print(f\"      - æ­£ç±» (å†å…¥é™¢): {(y_train==1).sum():,} ({(y_train==1).sum()/len(y_train)*100:.2f}%)\")\n",
    "print(f\"      - è´Ÿç±» (æ— å†å…¥é™¢): {(y_train==0).sum():,} ({(y_train==0).sum()/len(y_train)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n   æµ‹è¯•é›†: {X_test.shape[0]:,} æ ·æœ¬\")\n",
    "print(f\"      - æ­£ç±» (å†å…¥é™¢): {(y_test==1).sum():,} ({(y_test==1).sum()/len(y_test)*100:.2f}%)\")\n",
    "print(f\"      - è´Ÿç±» (æ— å†å…¥é™¢): {(y_test==0).sum():,} ({(y_test==0).sum()/len(y_test)*100:.2f}%)\")\n",
    "\n",
    "# 2ï¸âƒ£ Feature scaling\n",
    "print(\"\\n2ï¸âƒ£ ç‰¹å¾ç¼©æ”¾ (StandardScaler)...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(f\"   âœ… ç¼©æ”¾å®Œæˆ\")\n",
    "print(f\"   è®­ç»ƒé›†ç»Ÿè®¡ (ç¼©æ”¾å):\")\n",
    "print(f\"      - å‡å€¼: {X_train_scaled.mean().mean():.6f} (â‰ˆ 0 âœ“)\")\n",
    "print(f\"      - æ ‡å‡†å·®: {X_train_scaled.std().mean():.6f} (â‰ˆ 1 âœ“)\")\n",
    "\n",
    "print(\"\\nâœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e6b37",
   "metadata": {},
   "source": [
    "## Section 5-7: Build, Train, and Predict with Multiple Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aeb9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ã€Sections 5-7ã€‘æ„å»ºã€è®­ç»ƒå’Œé¢„æµ‹ - å¤šä¸ªåŸºçº¿æ¨¡å‹\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹...\")\n",
    "\n",
    "model_configs = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, solver='lbfgs', class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=10, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, scale_pos_weight=(y_train==0).sum()/(y_train==1).sum(), n_jobs=-1, eval_metric='logloss'),\n",
    "    'SVM': SVC(kernel='rbf', C=1.0, random_state=42, probability=True, class_weight='balanced'),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "for name, model in model_configs.items():\n",
    "    models[name] = model\n",
    "    print(f\"   âœ… {name} å·²åˆå§‹åŒ–\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ è®­ç»ƒæ¨¡å‹...\")\n",
    "for name, model in models.items():\n",
    "    print(f\"   è®­ç»ƒ {name}...\", end=\" \", flush=True)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    print(\"âœ…\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ ç”Ÿæˆé¢„æµ‹...\")\n",
    "for name, model in models.items():\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    y_train_pred_proba = model.predict_proba(X_train_scaled)[:, 1]\n",
    "    y_test_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    results[name] = {\n",
    "        'y_train_pred': y_train_pred,\n",
    "        'y_test_pred': y_test_pred,\n",
    "        'y_train_pred_proba': y_train_pred_proba,\n",
    "        'y_test_pred_proba': y_test_pred_proba,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "print(\"   âœ… æ‰€æœ‰æ¨¡å‹é¢„æµ‹å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19cfba4",
   "metadata": {},
   "source": [
    "## Section 8: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb42689",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ã€Section 8ã€‘æ¨¡å‹æ€§èƒ½è¯„ä¼°\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "metrics_summary = {}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"{'æ¨¡å‹':<20} {'AUROC':<12} {'AUPRC':<12} {'F1 Score':<12} {'Accuracy':<12} {'Precision':<12} {'Recall':<12}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for name, result in results.items():\n",
    "    y_test_pred_proba = result['y_test_pred_proba']\n",
    "    y_test_pred = result['y_test_pred']\n",
    "    \n",
    "    auroc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_test_pred_proba)\n",
    "    auprc = auc(recall_curve, precision_curve)\n",
    "    f1 = f1_score(y_test, y_test_pred)\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "    \n",
    "    metrics_summary[name] = {\n",
    "        'AUROC': auroc,\n",
    "        'AUPRC': auprc,\n",
    "        'F1': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'y_test_pred': y_test_pred,\n",
    "        'y_test_pred_proba': y_test_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:<20} {auroc:<12.4f} {auprc:<12.4f} {f1:<12.4f} {accuracy:<12.4f} {precision:<12.4f} {recall:<12.4f}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "\n",
    "best_model_name = max(metrics_summary, key=lambda x: metrics_summary[x]['AUROC'])\n",
    "print(f\"\\nğŸ† æœ€ä½³æ¨¡å‹ (æŒ‰ AUROC): {best_model_name}\")\n",
    "print(f\"   AUROC: {metrics_summary[best_model_name]['AUROC']:.4f}\")\n",
    "print(f\"   AUPRC: {metrics_summary[best_model_name]['AUPRC']:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… æ€§èƒ½è¯„ä¼°å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73be8d",
   "metadata": {},
   "source": [
    "## Section 9: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ecc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ã€Section 9ã€‘å¯è§†åŒ–ç»“æœ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. ROC Curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "for name, metrics in metrics_summary.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, metrics['y_test_pred_proba'])\n",
    "    ax.plot(fpr, tpr, label=f\"{name} (AUROC={metrics['AUROC']:.4f})\", linewidth=2)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "metrics_names = list(metrics_summary.keys())\n",
    "auroc_scores = [metrics_summary[name]['AUROC'] for name in metrics_names]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(metrics_names)))\n",
    "\n",
    "bars = ax.barh(metrics_names, auroc_scores, color=colors)\n",
    "ax.set_xlabel('AUROC Score', fontsize=12)\n",
    "ax.set_title('Model Comparison by AUROC', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([0.5, 1.0])\n",
    "\n",
    "for i, (bar, score) in enumerate(zip(bars, auroc_scores)):\n",
    "    ax.text(score + 0.01, i, f'{score:.4f}', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(data_dir, 'roc_and_auroc_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… ROC æ›²çº¿å’Œ AUROC å¯¹æ¯”å·²ä¿å­˜\")\n",
    "\n",
    "# 2. Confusion Matrices\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, metrics) in enumerate(sorted(metrics_summary.items())):\n",
    "    cm = confusion_matrix(y_test, metrics['y_test_pred'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], cbar=False,\n",
    "                xticklabels=['æ— å†å…¥é™¢', '30å¤©å†å…¥é™¢'],\n",
    "                yticklabels=['æ— å†å…¥é™¢', '30å¤©å†å…¥é™¢'])\n",
    "    axes[idx].set_title(f'{name}\\n(F1={metrics[\"F1\"]:.4f})', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(data_dir, 'confusion_matrices.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜\")\n",
    "\n",
    "# 3. Performance Metrics Heatmap\n",
    "metrics_df = pd.DataFrame(metrics_summary).T\n",
    "metrics_cols = ['AUROC', 'AUPRC', 'F1', 'Accuracy', 'Precision', 'Recall']\n",
    "metrics_data = metrics_df[metrics_cols]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.heatmap(metrics_data, annot=True, fmt='.4f', cmap='RdYlGn', ax=ax, \n",
    "            cbar_kws={'label': 'Score'}, vmin=0, vmax=1, linewidths=1, linecolor='gray')\n",
    "\n",
    "ax.set_title('Model Performance Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Metrics', fontsize=12)\n",
    "ax.set_ylabel('Models', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(data_dir, 'metrics_heatmap.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… æ€§èƒ½æŒ‡æ ‡çƒ­åŠ›å›¾å·²ä¿å­˜\")\n",
    "\n",
    "# 4. Feature Importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "rf_model = results['Random Forest']['model']\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.barh(range(len(feature_importance_rf)), feature_importance_rf['importance'], color='steelblue')\n",
    "ax.set_yticks(range(len(feature_importance_rf)))\n",
    "ax.set_yticklabels(feature_importance_rf['feature'])\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title('Random Forest - Top 20 Feature Importance', fontsize=12, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "xgb_model = results['XGBoost']['model']\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.barh(range(len(feature_importance_xgb)), feature_importance_xgb['importance'], color='coral')\n",
    "ax.set_yticks(range(len(feature_importance_xgb)))\n",
    "ax.set_yticklabels(feature_importance_xgb['feature'])\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title('XGBoost - Top 20 Feature Importance', fontsize=12, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(data_dir, 'feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜\")\n",
    "\n",
    "print(\"\\nâœ… æ‰€æœ‰å¯è§†åŒ–å·²å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47958ffe",
   "metadata": {},
   "source": [
    "## Section 10: Cross-Validation and Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f0f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ã€Section 10ã€‘äº¤å‰éªŒè¯å’Œæ€»ç»“\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ è¿›è¡Œ 5-Fold äº¤å‰éªŒè¯...\")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"   {name}...\", end=\" \", flush=True)\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "    cv_results[name] = {'mean': cv_scores.mean(), 'std': cv_scores.std(), 'scores': cv_scores}\n",
    "    print(f\"AUROC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ æµ‹è¯•é›†æ€§èƒ½ vs äº¤å‰éªŒè¯æ€§èƒ½:\")\n",
    "print(\"=\" * 110)\n",
    "print(f\"{'æ¨¡å‹':<20} {'æµ‹è¯•é›†AUROC':<18} {'äº¤å‰éªŒè¯AUROC':<22} {'å·®å¼‚':<12} {'æ ‡å‡†å·®':<12}\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "for name in sorted(metrics_summary.keys()):\n",
    "    test_auroc = metrics_summary[name]['AUROC']\n",
    "    cv_mean = cv_results[name]['mean']\n",
    "    cv_std = cv_results[name]['std']\n",
    "    diff = test_auroc - cv_mean\n",
    "    print(f\"{name:<20} {test_auroc:<18.4f} {cv_mean:<22.4f} {diff:<12.4f} {cv_std:<12.4f}\")\n",
    "\n",
    "print(\"=\" * 110)\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ ä¿å­˜ç»¼åˆæŠ¥å‘Š...\")\n",
    "\n",
    "report = f\"\"\"\n",
    "{'='*100}\n",
    "MIMIC-IV 30æ—¥å†å…¥é™¢é¢„æµ‹ - åŸºçº¿æ¨¡å‹æŠ¥å‘Š\n",
    "{'='*100}\n",
    "\n",
    "ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "ã€æ•°æ®é›†æ¦‚è§ˆã€‘\n",
    "{'-'*100}\n",
    "- æ€»æ ·æœ¬æ•°: {len(X):,}\n",
    "- ç‰¹å¾ç»´åº¦: {X.shape[1]}\n",
    "- æ­£ç±»æ¯”ä¾‹: {(y==1).sum()/len(y)*100:.2f}%\n",
    "- è®­ç»ƒé›†å¤§å°: {len(X_train):,}\n",
    "- æµ‹è¯•é›†å¤§å°: {len(X_test):,}\n",
    "\n",
    "ã€æµ‹è¯•é›†æ€§èƒ½æŒ‡æ ‡ã€‘\n",
    "{'-'*100}\n",
    "{'æ¨¡å‹':<20} {'AUROC':<12} {'AUPRC':<12} {'F1 Score':<12} {'Accuracy':<12} {'Precision':<12} {'Recall':<12}\n",
    "{'-'*100}\n",
    "\"\"\"\n",
    "\n",
    "for name in sorted(metrics_summary.keys()):\n",
    "    m = metrics_summary[name]\n",
    "    report += f\"{name:<20} {m['AUROC']:<12.4f} {m['AUPRC']:<12.4f} {m['F1']:<12.4f} {m['Accuracy']:<12.4f} {m['Precision']:<12.4f} {m['Recall']:<12.4f}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "ã€äº¤å‰éªŒè¯ç»“æœ (5-Fold)ã€‘\n",
    "{'-'*100}\n",
    "{'æ¨¡å‹':<20} {'å¹³å‡AUROC':<18} {'æ ‡å‡†å·®':<12}\n",
    "{'-'*100}\n",
    "\"\"\"\n",
    "\n",
    "for name in sorted(cv_results.keys()):\n",
    "    cv = cv_results[name]\n",
    "    report += f\"{name:<20} {cv['mean']:<18.4f} {cv['std']:<12.4f}\\n\"\n",
    "\n",
    "best_model = max(metrics_summary, key=lambda x: metrics_summary[x]['AUROC'])\n",
    "report += f\"\"\"\n",
    "ã€æœ€ä½³æ¨¡å‹ã€‘\n",
    "{'-'*100}\n",
    "æ¨¡å‹: {best_model}\n",
    "æµ‹è¯•é›† AUROC: {metrics_summary[best_model]['AUROC']:.4f}\n",
    "æµ‹è¯•é›† AUPRC: {metrics_summary[best_model]['AUPRC']:.4f}\n",
    "æµ‹è¯•é›† F1 Score: {metrics_summary[best_model]['F1']:.4f}\n",
    "äº¤å‰éªŒè¯ AUROC: {cv_results[best_model]['mean']:.4f} (+/- {cv_results[best_model]['std']:.4f})\n",
    "\n",
    "ã€å»ºè®®å’Œåç»­æ­¥éª¤ã€‘\n",
    "{'-'*100}\n",
    "1. è¶…å‚æ•°è°ƒä¼˜ - GridSearchCV æˆ– RandomizedSearchCV\n",
    "2. ç‰¹å¾å·¥ç¨‹ - ç‰¹å¾äº¤äº’ã€å¤šé¡¹å¼ç‰¹å¾\n",
    "3. æ•°æ®å¹³è¡¡ - SMOTE è¿‡é‡‡æ ·\n",
    "4. é›†æˆå­¦ä¹  - Stacking/Voting\n",
    "5. æ¨¡å‹è§£é‡Š - SHAP values, LIME\n",
    "\n",
    "{'='*100}\n",
    "\"\"\"\n",
    "\n",
    "report_path = os.path.join(data_dir, 'baseline_models_report.txt')\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"   âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_path}\")\n",
    "\n",
    "metrics_json = {}\n",
    "for name, metrics in metrics_summary.items():\n",
    "    metrics_json[name] = {\n",
    "        'AUROC': float(metrics['AUROC']),\n",
    "        'AUPRC': float(metrics['AUPRC']),\n",
    "        'F1': float(metrics['F1']),\n",
    "        'Accuracy': float(metrics['Accuracy']),\n",
    "        'Precision': float(metrics['Precision']),\n",
    "        'Recall': float(metrics['Recall']),\n",
    "        'CV_Mean': float(cv_results[name]['mean']),\n",
    "        'CV_Std': float(cv_results[name]['std'])\n",
    "    }\n",
    "\n",
    "json_path = os.path.join(data_dir, 'baseline_models_metrics.json')\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"   âœ… æŒ‡æ ‡å·²ä¿å­˜: {json_path}\")\n",
    "print(f\"\\nâœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {data_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
