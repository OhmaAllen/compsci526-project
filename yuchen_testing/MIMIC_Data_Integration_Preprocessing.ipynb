{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea05529b",
   "metadata": {},
   "source": [
    "# MIMIC-IV Data Integration & Preprocessing Pipeline\n",
    "## 30-Day Readmission Prediction\n",
    "\n",
    "**Author**: Data Processing Pipeline  \n",
    "**Date**: 2025-10-16  \n",
    "**Objective**: Integrate multiple MIMIC-IV tables and generate features for 30-day readmission prediction\n",
    "\n",
    "**Pipeline Steps**:\n",
    "1. Load core tables (admissions, patients, diagnoses, procedures, etc.)\n",
    "2. Define 30-day readmission target\n",
    "3. Extract demographic, clinical, and temporal features\n",
    "4. Handle missing values and data quality issues\n",
    "5. Save cleaned dataset for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be070d3",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ef0d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n",
      "üìÅ MIMIC data directory: /Users/yuchenzhou/Documents/duke/compsci526/final_proj/mimic_data\n",
      "üìÅ Output directory: /Users/yuchenzhou/Documents/duke/compsci526/final_proj/mimic_data/processed_data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define data directories\n",
    "mimic_data_dir = '/Users/yuchenzhou/Documents/duke/compsci526/final_proj/mimic_data'\n",
    "output_dir = '/Users/yuchenzhou/Documents/duke/compsci526/final_proj/mimic_data/processed_data'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìÅ MIMIC data directory: {mimic_data_dir}\")\n",
    "print(f\"üìÅ Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1db4868",
   "metadata": {},
   "source": [
    "## Step 2: Load Core Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d00ce4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Loading MIMIC-IV Core Tables\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Loading admissions...\n",
      "   ‚úÖ Admissions: (546028, 16)\n",
      "\n",
      "2Ô∏è‚É£ Loading patients...\n",
      "   ‚úÖ Patients: (364627, 6)\n",
      "\n",
      "3Ô∏è‚É£ Loading diagnoses_icd...\n",
      "   ‚úÖ Admissions: (546028, 16)\n",
      "\n",
      "2Ô∏è‚É£ Loading patients...\n",
      "   ‚úÖ Patients: (364627, 6)\n",
      "\n",
      "3Ô∏è‚É£ Loading diagnoses_icd...\n",
      "   ‚úÖ Diagnoses: (6364488, 5)\n",
      "\n",
      "4Ô∏è‚É£ Loading procedures_icd...\n",
      "   ‚úÖ Diagnoses: (6364488, 5)\n",
      "\n",
      "4Ô∏è‚É£ Loading procedures_icd...\n",
      "   ‚úÖ Procedures: (859655, 6)\n",
      "\n",
      "5Ô∏è‚É£ Loading labevents (sampling first 1M rows)...\n",
      "   ‚úÖ Procedures: (859655, 6)\n",
      "\n",
      "5Ô∏è‚É£ Loading labevents (sampling first 1M rows)...\n",
      "   ‚úÖ Labevents: (1000000, 16)\n",
      "\n",
      "‚úÖ Core tables loaded successfully\n",
      "   ‚úÖ Labevents: (1000000, 16)\n",
      "\n",
      "‚úÖ Core tables loaded successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Loading MIMIC-IV Core Tables\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load admissions\n",
    "print(\"\\n1Ô∏è‚É£ Loading admissions...\")\n",
    "admissions = pd.read_csv(os.path.join(mimic_data_dir, 'admissions.csv'))\n",
    "print(f\"   ‚úÖ Admissions: {admissions.shape}\")\n",
    "\n",
    "# Load patients\n",
    "print(\"\\n2Ô∏è‚É£ Loading patients...\")\n",
    "patients = pd.read_csv(os.path.join(mimic_data_dir, 'patients.csv'))\n",
    "print(f\"   ‚úÖ Patients: {patients.shape}\")\n",
    "\n",
    "# Load diagnoses_icd\n",
    "print(\"\\n3Ô∏è‚É£ Loading diagnoses_icd...\")\n",
    "diagnoses = pd.read_csv(os.path.join(mimic_data_dir, 'diagnoses_icd.csv'))\n",
    "print(f\"   ‚úÖ Diagnoses: {diagnoses.shape}\")\n",
    "\n",
    "# Load procedures_icd\n",
    "print(\"\\n4Ô∏è‚É£ Loading procedures_icd...\")\n",
    "procedures = pd.read_csv(os.path.join(mimic_data_dir, 'procedures_icd.csv'))\n",
    "print(f\"   ‚úÖ Procedures: {procedures.shape}\")\n",
    "\n",
    "# Load labevents (sample)\n",
    "print(\"\\n5Ô∏è‚É£ Loading labevents (sampling first 1M rows)...\")\n",
    "try:\n",
    "    labevents = pd.read_csv(os.path.join(mimic_data_dir, 'labevents.csv'), nrows=1000000)\n",
    "    print(f\"   ‚úÖ Labevents: {labevents.shape}\")\n",
    "except:\n",
    "    print(\"   ‚ö†Ô∏è Labevents not found or too large, skipping...\")\n",
    "    labevents = None\n",
    "\n",
    "print(\"\\n‚úÖ Core tables loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57baeb5",
   "metadata": {},
   "source": [
    "## Step 3: Define 30-Day Readmission Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab7e5eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Defining 30-Day Readmission Target\n",
      "================================================================================\n",
      "\n",
      "üìä Readmission Statistics:\n",
      "   Total admissions: 546,028\n",
      "   30-day readmissions: 109,345\n",
      "   Readmission rate: 20.03%\n",
      "\n",
      "‚úÖ Target variable created\n",
      "\n",
      "üìä Readmission Statistics:\n",
      "   Total admissions: 546,028\n",
      "   30-day readmissions: 109,345\n",
      "   Readmission rate: 20.03%\n",
      "\n",
      "‚úÖ Target variable created\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Defining 30-Day Readmission Target\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Convert datetime columns\n",
    "admissions['admittime'] = pd.to_datetime(admissions['admittime'])\n",
    "admissions['dischtime'] = pd.to_datetime(admissions['dischtime'])\n",
    "\n",
    "# Sort by subject_id and admission time\n",
    "admissions_sorted = admissions.sort_values(['subject_id', 'admittime']).reset_index(drop=True)\n",
    "\n",
    "# Calculate time to next admission\n",
    "admissions_sorted['next_admittime'] = admissions_sorted.groupby('subject_id')['admittime'].shift(-1)\n",
    "admissions_sorted['days_to_readmit'] = (\n",
    "    admissions_sorted['next_admittime'] - admissions_sorted['dischtime']\n",
    ").dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "# Define 30-day readmission\n",
    "admissions_sorted['readmit_30d'] = (\n",
    "    (admissions_sorted['days_to_readmit'] >= 0) & \n",
    "    (admissions_sorted['days_to_readmit'] <= 30)\n",
    ").astype(int)\n",
    "\n",
    "# Statistics\n",
    "total_admissions = len(admissions_sorted)\n",
    "readmitted = admissions_sorted['readmit_30d'].sum()\n",
    "readmit_rate = (readmitted / total_admissions) * 100\n",
    "\n",
    "print(f\"\\nüìä Readmission Statistics:\")\n",
    "print(f\"   Total admissions: {total_admissions:,}\")\n",
    "print(f\"   30-day readmissions: {readmitted:,}\")\n",
    "print(f\"   Readmission rate: {readmit_rate:.2f}%\")\n",
    "\n",
    "print(\"\\n‚úÖ Target variable created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1334f3b",
   "metadata": {},
   "source": [
    "## Step 4: Extract Demographic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2686ba13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Extracting Demographic Features\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Demographic features extracted\n",
      "   Current shape: (546028, 89)\n",
      "\n",
      "‚úÖ Demographic features extracted\n",
      "   Current shape: (546028, 89)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Extracting Demographic Features\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Merge with patients table\n",
    "df = admissions_sorted.merge(patients, on='subject_id', how='left')\n",
    "\n",
    "# Calculate age at admission\n",
    "df['anchor_year_group'] = df['anchor_year_group'].str.split('-').str[0].astype(int)\n",
    "df['age'] = df['anchor_year_group'] + df['anchor_age'] - df['anchor_year']\n",
    "\n",
    "# Gender encoding\n",
    "df['gender_M'] = (df['gender'] == 'M').astype(int)\n",
    "df['gender_F'] = (df['gender'] == 'F').astype(int)\n",
    "\n",
    "# Admission type encoding\n",
    "admission_types = pd.get_dummies(df['admission_type'], prefix='admission')\n",
    "df = pd.concat([df, admission_types], axis=1)\n",
    "\n",
    "# Admission location encoding\n",
    "admission_loc = pd.get_dummies(df['admission_location'], prefix='admit_loc')\n",
    "df = pd.concat([df, admission_loc], axis=1)\n",
    "\n",
    "# Insurance encoding\n",
    "insurance = pd.get_dummies(df['insurance'], prefix='insurance')\n",
    "df = pd.concat([df, insurance], axis=1)\n",
    "\n",
    "# Marital status encoding  \n",
    "marital = pd.get_dummies(df['marital_status'], prefix='marital')\n",
    "df = pd.concat([df, marital], axis=1)\n",
    "\n",
    "# Race/Ethnicity encoding\n",
    "race = pd.get_dummies(df['race'], prefix='race')\n",
    "df = pd.concat([df, race], axis=1)\n",
    "\n",
    "print(f\"\\n‚úÖ Demographic features extracted\")\n",
    "print(f\"   Current shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c441d",
   "metadata": {},
   "source": [
    "## Step 5: Extract Clinical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aacd070b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Extracting Clinical Features\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Counting diagnoses per admission...\n",
      "\n",
      "2Ô∏è‚É£ Counting procedures per admission...\n",
      "\n",
      "3Ô∏è‚É£ Extracting primary diagnosis...\n",
      "\n",
      "2Ô∏è‚É£ Counting procedures per admission...\n",
      "\n",
      "3Ô∏è‚É£ Extracting primary diagnosis...\n",
      "\n",
      "‚úÖ Clinical features extracted\n",
      "   Current shape: (546028, 106)\n",
      "\n",
      "‚úÖ Clinical features extracted\n",
      "   Current shape: (546028, 106)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Extracting Clinical Features\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Length of stay\n",
    "df['los_days'] = (df['dischtime'] - df['admittime']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "# Hospital expire flag\n",
    "df['hospital_expire_flag'] = df['hospital_expire_flag'].fillna(0).astype(int)\n",
    "\n",
    "# Number of diagnoses\n",
    "print(\"\\n1Ô∏è‚É£ Counting diagnoses per admission...\")\n",
    "diag_counts = diagnoses.groupby('hadm_id').size().reset_index(name='num_diagnoses')\n",
    "df = df.merge(diag_counts, on='hadm_id', how='left')\n",
    "df['num_diagnoses'] = df['num_diagnoses'].fillna(0)\n",
    "\n",
    "# Number of procedures\n",
    "print(\"\\n2Ô∏è‚É£ Counting procedures per admission...\")\n",
    "proc_counts = procedures.groupby('hadm_id').size().reset_index(name='num_procedures')\n",
    "df = df.merge(proc_counts, on='hadm_id', how='left')\n",
    "df['num_procedures'] = df['num_procedures'].fillna(0)\n",
    "\n",
    "# Primary diagnosis (first diagnosis code)\n",
    "print(\"\\n3Ô∏è‚É£ Extracting primary diagnosis...\")\n",
    "primary_diag = diagnoses.sort_values(['hadm_id', 'seq_num']).groupby('hadm_id').first().reset_index()\n",
    "primary_diag = primary_diag[['hadm_id', 'icd_code']]\n",
    "primary_diag.columns = ['hadm_id', 'primary_icd_code']\n",
    "df = df.merge(primary_diag, on='hadm_id', how='left')\n",
    "\n",
    "# Discharge location encoding\n",
    "discharge_loc = pd.get_dummies(df['discharge_location'], prefix='discharge')\n",
    "df = pd.concat([df, discharge_loc], axis=1)\n",
    "\n",
    "print(f\"\\n‚úÖ Clinical features extracted\")\n",
    "print(f\"   Current shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc42c65",
   "metadata": {},
   "source": [
    "## Step 6: Extract Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7504842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Extracting Temporal Features\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Counting previous admissions...\n",
      "\n",
      "2Ô∏è‚É£ Calculating days since last admission...\n",
      "\n",
      "‚úÖ Temporal features extracted\n",
      "   Current shape: (546028, 118)\n",
      "\n",
      "2Ô∏è‚É£ Calculating days since last admission...\n",
      "\n",
      "‚úÖ Temporal features extracted\n",
      "   Current shape: (546028, 118)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Extracting Temporal Features\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Admission year, month, day of week\n",
    "df['admit_year'] = df['admittime'].dt.year\n",
    "df['admit_month'] = df['admittime'].dt.month\n",
    "df['admit_dow'] = df['admittime'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df['admit_hour'] = df['admittime'].dt.hour\n",
    "\n",
    "# Weekend admission\n",
    "df['admit_weekend'] = (df['admit_dow'] >= 5).astype(int)\n",
    "\n",
    "# Season\n",
    "df['admit_season'] = pd.cut(df['admit_month'], \n",
    "                              bins=[0, 3, 6, 9, 12],\n",
    "                              labels=['Winter', 'Spring', 'Summer', 'Fall'])\n",
    "season_dummies = pd.get_dummies(df['admit_season'], prefix='season')\n",
    "df = pd.concat([df, season_dummies], axis=1)\n",
    "\n",
    "# Previous admissions count\n",
    "print(\"\\n1Ô∏è‚É£ Counting previous admissions...\")\n",
    "df_sorted = df.sort_values(['subject_id', 'admittime'])\n",
    "df_sorted['prev_admissions'] = df_sorted.groupby('subject_id').cumcount()\n",
    "df['prev_admissions'] = df_sorted['prev_admissions']\n",
    "\n",
    "# Days since last admission\n",
    "print(\"\\n2Ô∏è‚É£ Calculating days since last admission...\")\n",
    "df_sorted['prev_dischtime'] = df_sorted.groupby('subject_id')['dischtime'].shift(1)\n",
    "df_sorted['days_since_last_admission'] = (\n",
    "    df_sorted['admittime'] - df_sorted['prev_dischtime']\n",
    ").dt.total_seconds() / (24 * 3600)\n",
    "df['days_since_last_admission'] = df_sorted['days_since_last_admission'].fillna(-1)\n",
    "\n",
    "print(f\"\\n‚úÖ Temporal features extracted\")\n",
    "print(f\"   Current shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d348cc",
   "metadata": {},
   "source": [
    "## Step 7: Data Cleaning and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e6e8877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Data Cleaning and Quality Checks\n",
      "================================================================================\n",
      "\n",
      "üìä Initial dataset: (546028, 118)\n",
      "\n",
      "1Ô∏è‚É£ Removing rows with missing target...\n",
      "   Removed 0 rows with missing target\n",
      "\n",
      "2Ô∏è‚É£ Filtering age range...\n",
      "   Kept admissions for ages 18-120\n",
      "\n",
      "3Ô∏è‚É£ Filtering length of stay...\n",
      "   Removed admissions with LOS <= 0\n",
      "\n",
      "4Ô∏è‚É£ Handling missing values...\n",
      "   Missing values per column:\n",
      "      No missing values!\n",
      "\n",
      "‚úÖ Data cleaning complete\n",
      "   Final dataset: (0, 118)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Data Cleaning and Quality Checks\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Initial dataset: {df.shape}\")\n",
    "\n",
    "# Remove rows with missing target\n",
    "print(\"\\n1Ô∏è‚É£ Removing rows with missing target...\")\n",
    "initial_rows = len(df)\n",
    "df = df[df['readmit_30d'].notna()]\n",
    "removed = initial_rows - len(df)\n",
    "print(f\"   Removed {removed} rows with missing target\")\n",
    "\n",
    "# Remove unrealistic ages\n",
    "print(\"\\n2Ô∏è‚É£ Filtering age range...\")\n",
    "df = df[(df['age'] >= 18) & (df['age'] <= 120)]\n",
    "print(f\"   Kept admissions for ages 18-120\")\n",
    "\n",
    "# Remove negative or zero LOS\n",
    "print(\"\\n3Ô∏è‚É£ Filtering length of stay...\")\n",
    "df = df[df['los_days'] > 0]\n",
    "print(f\"   Removed admissions with LOS <= 0\")\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\n4Ô∏è‚É£ Handling missing values...\")\n",
    "print(f\"   Missing values per column:\")\n",
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "if len(missing) > 0:\n",
    "    for col, count in missing.head(10).items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"      {col}: {count} ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"      No missing values!\")\n",
    "\n",
    "# Fill numeric missing with median\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# Fill categorical missing with 'Unknown'\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna('Unknown', inplace=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Data cleaning complete\")\n",
    "print(f\"   Final dataset: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca29123",
   "metadata": {},
   "source": [
    "## Step 8: Select Final Features and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfc88737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Selecting Features and Saving Dataset\n",
      "================================================================================\n",
      "\n",
      "üìä Final Dataset Summary:\n",
      "   Shape: (0, 101)\n",
      "   Features: 100\n",
      "   Target variable: readmit_30d\n",
      "   Readmission rate: nan%\n",
      "\n",
      "‚úÖ Dataset saved to: /Users/yuchenzhou/Documents/duke/compsci526/final_proj/mimic_data/processed_data/readmission_features_cleaned.csv\n",
      "\n",
      "üìã First 5 rows:\n",
      "Empty DataFrame\n",
      "Columns: [subject_id, hadm_id, readmit_30d, age, los_days, num_diagnoses, num_procedures, prev_admissions, days_since_last_admission, admit_year, admit_month, admit_dow, admit_hour, admit_weekend, hospital_expire_flag, primary_icd_code, gender_M, gender_F, admission_type, admission_location, admission_AMBULATORY OBSERVATION, admission_DIRECT EMER., admission_DIRECT OBSERVATION, admission_ELECTIVE, admission_EU OBSERVATION, admission_EW EMER., admission_OBSERVATION ADMIT, admission_SURGICAL SAME DAY ADMISSION, admission_URGENT, admit_loc_AMBULATORY SURGERY TRANSFER, admit_loc_CLINIC REFERRAL, admit_loc_EMERGENCY ROOM, admit_loc_INFORMATION NOT AVAILABLE, admit_loc_INTERNAL TRANSFER TO OR FROM PSYCH, admit_loc_PACU, admit_loc_PHYSICIAN REFERRAL, admit_loc_PROCEDURE SITE, admit_loc_TRANSFER FROM HOSPITAL, admit_loc_TRANSFER FROM SKILLED NURSING FACILITY, admit_loc_WALK-IN/SELF REFERRAL, insurance_Medicaid, insurance_Medicare, insurance_No charge, insurance_Other, insurance_Private, marital_status, marital_DIVORCED, marital_MARRIED, marital_SINGLE, marital_WIDOWED, race_AMERICAN INDIAN/ALASKA NATIVE, race_ASIAN, race_ASIAN - ASIAN INDIAN, race_ASIAN - CHINESE, race_ASIAN - KOREAN, race_ASIAN - SOUTH EAST ASIAN, race_BLACK/AFRICAN, race_BLACK/AFRICAN AMERICAN, race_BLACK/CAPE VERDEAN, race_BLACK/CARIBBEAN ISLAND, race_HISPANIC OR LATINO, race_HISPANIC/LATINO - CENTRAL AMERICAN, race_HISPANIC/LATINO - COLUMBIAN, race_HISPANIC/LATINO - CUBAN, race_HISPANIC/LATINO - DOMINICAN, race_HISPANIC/LATINO - GUATEMALAN, race_HISPANIC/LATINO - HONDURAN, race_HISPANIC/LATINO - MEXICAN, race_HISPANIC/LATINO - PUERTO RICAN, race_HISPANIC/LATINO - SALVADORAN, race_MULTIPLE RACE/ETHNICITY, race_NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER, race_OTHER, race_PATIENT DECLINED TO ANSWER, race_PORTUGUESE, race_SOUTH AMERICAN, race_UNABLE TO OBTAIN, race_UNKNOWN, race_WHITE, race_WHITE - BRAZILIAN, race_WHITE - EASTERN EUROPEAN, race_WHITE - OTHER EUROPEAN, race_WHITE - RUSSIAN, discharge_location, discharge_ACUTE HOSPITAL, discharge_AGAINST ADVICE, discharge_ASSISTED LIVING, discharge_CHRONIC/LONG TERM ACUTE CARE, discharge_DIED, discharge_HEALTHCARE FACILITY, discharge_HOME, discharge_HOME HEALTH CARE, discharge_HOSPICE, discharge_OTHER FACILITY, discharge_PSYCH FACILITY, discharge_REHAB, discharge_SKILLED NURSING FACILITY, season_Winter, season_Spring, season_Summer, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Selecting Features and Saving Dataset\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select key columns to keep\n",
    "columns_to_keep = ['subject_id', 'hadm_id', 'readmit_30d', 'age', 'los_days', \n",
    "                   'num_diagnoses', 'num_procedures', 'prev_admissions',\n",
    "                   'days_since_last_admission', 'admit_year', 'admit_month', \n",
    "                   'admit_dow', 'admit_hour', 'admit_weekend', 'hospital_expire_flag',\n",
    "                   'primary_icd_code']\n",
    "\n",
    "# Add all one-hot encoded columns\n",
    "encoded_prefixes = ['gender', 'admission', 'admit_loc', 'insurance', 'marital', \n",
    "                    'race', 'discharge', 'season']\n",
    "for prefix in encoded_prefixes:\n",
    "    encoded_cols = [col for col in df.columns if col.startswith(prefix + '_')]\n",
    "    columns_to_keep.extend(encoded_cols)\n",
    "\n",
    "# Keep only columns that exist\n",
    "columns_to_keep = [col for col in columns_to_keep if col in df.columns]\n",
    "\n",
    "# Create final dataset\n",
    "df_final = df[columns_to_keep].copy()\n",
    "\n",
    "print(f\"\\nüìä Final Dataset Summary:\")\n",
    "print(f\"   Shape: {df_final.shape}\")\n",
    "print(f\"   Features: {df_final.shape[1] - 1}\")  # Excluding target\n",
    "print(f\"   Target variable: readmit_30d\")\n",
    "print(f\"   Readmission rate: {df_final['readmit_30d'].mean()*100:.2f}%\")\n",
    "\n",
    "# Save to CSV\n",
    "output_path = os.path.join(output_dir, 'readmission_features_cleaned.csv')\n",
    "df_final.to_csv(output_path, index=False)\n",
    "print(f\"\\n‚úÖ Dataset saved to: {output_path}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nüìã First 5 rows:\")\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6191b3e",
   "metadata": {},
   "source": [
    "## Step 9: Generate Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eef1d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Generating Summary Report\n",
      "================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "MIMIC-IV Data Integration & Preprocessing Report\n",
      "====================================================================================================\n",
      "\n",
      "Generated: 2025-10-18 23:33:27\n",
      "\n",
      "„ÄêDataset Overview„Äë\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Total admissions: 0\n",
      "Total features: 100\n",
      "Target variable: readmit_30d (30-day readmission)\n",
      "\n",
      "„ÄêTarget Distribution„Äë\n",
      "----------------------------------------------------------------------------------------------------\n",
      "No readmission (0): 0 (nan%)\n",
      "Readmitted (1): 0 (nan%)\n",
      "\n",
      "„ÄêFeature Categories„Äë\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Demographic features:\n",
      "  - Age, gender, race, marital status, insurance\n",
      "\n",
      "Clinical features:\n",
      "  - Length of stay, number of diagnoses, number of procedures\n",
      "  - Primary diagnosis code, hospital expire flag\n",
      "  - Admission type, admission location, discharge location\n",
      "\n",
      "Temporal features:\n",
      "  - Previous admissions count\n",
      "  - Days since last admission\n",
      "  - Admission time features (year, month, day of week, hour)\n",
      "  - Weekend admission, season\n",
      "\n",
      "„ÄêData Quality„Äë\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Missing values: 0\n",
      "Duplicate rows: 0\n",
      "\n",
      "„ÄêAge Distribution„Äë\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean age: nan years\n",
      "Median age: nan years\n",
      "Age range: nan - nan years\n",
      "\n",
      "„ÄêLength of Stay„Äë\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean LOS: nan days\n",
      "Median LOS: nan days\n",
      "\n",
      "„ÄêOutput Files„Äë\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cleaned dataset: /Users/yuchenzhou/Documents/duke/compsci526/final_proj/mimic_data/processed_data/readmission_features_cleaned.csv\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Report saved to: /Users/yuchenzhou/Documents/duke/compsci526/final_proj/mimic_data/processed_data/preprocessing_report.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Generating Summary Report\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "report = f\"\"\"\n",
    "{'='*100}\n",
    "MIMIC-IV Data Integration & Preprocessing Report\n",
    "{'='*100}\n",
    "\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "„ÄêDataset Overview„Äë\n",
    "{'-'*100}\n",
    "Total admissions: {len(df_final):,}\n",
    "Total features: {df_final.shape[1] - 1}\n",
    "Target variable: readmit_30d (30-day readmission)\n",
    "\n",
    "„ÄêTarget Distribution„Äë\n",
    "{'-'*100}\n",
    "No readmission (0): {(df_final['readmit_30d']==0).sum():,} ({(df_final['readmit_30d']==0).mean()*100:.2f}%)\n",
    "Readmitted (1): {(df_final['readmit_30d']==1).sum():,} ({(df_final['readmit_30d']==1).mean()*100:.2f}%)\n",
    "\n",
    "„ÄêFeature Categories„Äë\n",
    "{'-'*100}\n",
    "Demographic features:\n",
    "  - Age, gender, race, marital status, insurance\n",
    "  \n",
    "Clinical features:\n",
    "  - Length of stay, number of diagnoses, number of procedures\n",
    "  - Primary diagnosis code, hospital expire flag\n",
    "  - Admission type, admission location, discharge location\n",
    "  \n",
    "Temporal features:\n",
    "  - Previous admissions count\n",
    "  - Days since last admission\n",
    "  - Admission time features (year, month, day of week, hour)\n",
    "  - Weekend admission, season\n",
    "\n",
    "„ÄêData Quality„Äë\n",
    "{'-'*100}\n",
    "Missing values: {df_final.isnull().sum().sum()}\n",
    "Duplicate rows: {df_final.duplicated().sum()}\n",
    "\n",
    "„ÄêAge Distribution„Äë\n",
    "{'-'*100}\n",
    "Mean age: {df_final['age'].mean():.1f} years\n",
    "Median age: {df_final['age'].median():.1f} years\n",
    "Age range: {df_final['age'].min():.0f} - {df_final['age'].max():.0f} years\n",
    "\n",
    "„ÄêLength of Stay„Äë\n",
    "{'-'*100}\n",
    "Mean LOS: {df_final['los_days'].mean():.1f} days\n",
    "Median LOS: {df_final['los_days'].median():.1f} days\n",
    "\n",
    "„ÄêOutput Files„Äë\n",
    "{'-'*100}\n",
    "Cleaned dataset: {output_path}\n",
    "\n",
    "{'='*100}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "report_path = os.path.join(output_dir, 'preprocessing_report.txt')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"‚úÖ Report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c722a9",
   "metadata": {},
   "source": [
    "## üéâ Pipeline Complete!\n",
    "\n",
    "**Next Steps:**\n",
    "1. Open `baseline_models.ipynb` to build and evaluate baseline models\n",
    "2. The cleaned dataset is ready at: `/Users/yuchenzhou/Documents/duke/compsci526/final_proj/mimic_data/processed_data/readmission_features_cleaned.csv`\n",
    "3. Review the preprocessing report for data quality insights\n",
    "\n",
    "**Key Outputs:**\n",
    "- ‚úÖ `readmission_features_cleaned.csv` - Cleaned dataset for modeling\n",
    "- ‚úÖ `preprocessing_report.txt` - Data quality and statistics report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
