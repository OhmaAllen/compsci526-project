{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86cd756c",
   "metadata": {},
   "source": [
    "# Preprocessing + 30-day Readmission Notebook\n",
    "\n",
    "This focused notebook extracts and documents the preprocessing steps used to build the 30-day readmission label and the pooled static + time-series features. It is a smaller workspace for iterative experiments (safe to run on a dev machine).\n",
    "\n",
    "Key contents:\n",
    "- DATA_DIR setup and quick preview of CSVs\n",
    "- 30-day readmission label computation\n",
    "- Loading patients/diagnoses/chartevents/labevents (with date parsing)\n",
    "- Age computation using `anchor_age` and `anchor_year`\n",
    "- Static features and pooled time-series feature construction\n",
    "- Imputation, scaling, and simple CV training for baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6fdd967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR = /Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1\n",
      "--- admissions -> /Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1/hosp/admissions.csv\n",
      "columns: ['subject_id', 'hadm_id', 'admittime', 'dischtime', 'deathtime', 'admission_type', 'admit_provider_id', 'admission_location', 'discharge_location', 'insurance', 'language', 'marital_status', 'race', 'edregtime', 'edouttime', 'hospital_expire_flag']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>admittime</th>\n",
       "      <th>dischtime</th>\n",
       "      <th>deathtime</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>admit_provider_id</th>\n",
       "      <th>admission_location</th>\n",
       "      <th>discharge_location</th>\n",
       "      <th>insurance</th>\n",
       "      <th>language</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>race</th>\n",
       "      <th>edregtime</th>\n",
       "      <th>edouttime</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853</td>\n",
       "      <td>2180-05-06 22:23:00</td>\n",
       "      <td>2180-05-07 17:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>URGENT</td>\n",
       "      <td>P49AFC</td>\n",
       "      <td>TRANSFER FROM HOSPITAL</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Medicaid</td>\n",
       "      <td>English</td>\n",
       "      <td>WIDOWED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2180-05-06 19:17:00</td>\n",
       "      <td>2180-05-06 23:30:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22841357</td>\n",
       "      <td>2180-06-26 18:27:00</td>\n",
       "      <td>2180-06-27 18:49:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EW EMER.</td>\n",
       "      <td>P784FA</td>\n",
       "      <td>EMERGENCY ROOM</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Medicaid</td>\n",
       "      <td>English</td>\n",
       "      <td>WIDOWED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>25742920</td>\n",
       "      <td>2180-08-05 23:44:00</td>\n",
       "      <td>2180-08-07 17:50:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EW EMER.</td>\n",
       "      <td>P19UTS</td>\n",
       "      <td>EMERGENCY ROOM</td>\n",
       "      <td>HOSPICE</td>\n",
       "      <td>Medicaid</td>\n",
       "      <td>English</td>\n",
       "      <td>WIDOWED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id            admittime            dischtime  deathtime  \\\n",
       "0    10000032  22595853  2180-05-06 22:23:00  2180-05-07 17:15:00        NaN   \n",
       "1    10000032  22841357  2180-06-26 18:27:00  2180-06-27 18:49:00        NaN   \n",
       "2    10000032  25742920  2180-08-05 23:44:00  2180-08-07 17:50:00        NaN   \n",
       "\n",
       "  admission_type admit_provider_id      admission_location discharge_location  \\\n",
       "0         URGENT            P49AFC  TRANSFER FROM HOSPITAL               HOME   \n",
       "1       EW EMER.            P784FA          EMERGENCY ROOM               HOME   \n",
       "2       EW EMER.            P19UTS          EMERGENCY ROOM            HOSPICE   \n",
       "\n",
       "  insurance language marital_status   race            edregtime  \\\n",
       "0  Medicaid  English        WIDOWED  WHITE  2180-05-06 19:17:00   \n",
       "1  Medicaid  English        WIDOWED  WHITE  2180-06-26 15:54:00   \n",
       "2  Medicaid  English        WIDOWED  WHITE  2180-08-05 20:58:00   \n",
       "\n",
       "             edouttime  hospital_expire_flag  \n",
       "0  2180-05-06 23:30:00                     0  \n",
       "1  2180-06-26 21:31:00                     0  \n",
       "2  2180-08-06 01:44:00                     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- patients -> /Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1/hosp/patients.csv\n",
      "columns: ['subject_id', 'gender', 'anchor_age', 'anchor_year', 'anchor_year_group', 'dod']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>anchor_age</th>\n",
       "      <th>anchor_year</th>\n",
       "      <th>anchor_year_group</th>\n",
       "      <th>dod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>F</td>\n",
       "      <td>52</td>\n",
       "      <td>2180</td>\n",
       "      <td>2014 - 2016</td>\n",
       "      <td>2180-09-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000048</td>\n",
       "      <td>F</td>\n",
       "      <td>23</td>\n",
       "      <td>2126</td>\n",
       "      <td>2008 - 2010</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000058</td>\n",
       "      <td>F</td>\n",
       "      <td>33</td>\n",
       "      <td>2168</td>\n",
       "      <td>2020 - 2022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id gender  anchor_age  anchor_year anchor_year_group         dod\n",
       "0    10000032      F          52         2180       2014 - 2016  2180-09-09\n",
       "1    10000048      F          23         2126       2008 - 2010         NaN\n",
       "2    10000058      F          33         2168       2020 - 2022         NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- diagnoses_icd -> /Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1/hosp/diagnoses_icd.csv\n",
      "columns: ['subject_id', 'hadm_id', 'seq_num', 'icd_code', 'icd_version']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>seq_num</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>icd_version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853</td>\n",
       "      <td>1</td>\n",
       "      <td>5723</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853</td>\n",
       "      <td>2</td>\n",
       "      <td>78959</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853</td>\n",
       "      <td>3</td>\n",
       "      <td>5715</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id  seq_num  icd_code  icd_version\n",
       "0    10000032  22595853        1      5723            9\n",
       "1    10000032  22595853        2     78959            9\n",
       "2    10000032  22595853        3      5715            9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- chartevents -> /Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1/icu/chartevents.csv\n",
      "columns: ['subject_id', 'hadm_id', 'stay_id', 'caregiver_id', 'charttime', 'storetime', 'itemid', 'value', 'valuenum', 'valueuom', 'warning']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>caregiver_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>itemid</th>\n",
       "      <th>value</th>\n",
       "      <th>valuenum</th>\n",
       "      <th>valueuom</th>\n",
       "      <th>warning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034</td>\n",
       "      <td>39553978</td>\n",
       "      <td>18704</td>\n",
       "      <td>2180-07-23 12:36:00</td>\n",
       "      <td>2180-07-23 14:45:00</td>\n",
       "      <td>226512</td>\n",
       "      <td>39.4</td>\n",
       "      <td>39.4</td>\n",
       "      <td>kg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034</td>\n",
       "      <td>39553978</td>\n",
       "      <td>18704</td>\n",
       "      <td>2180-07-23 12:36:00</td>\n",
       "      <td>2180-07-23 14:45:00</td>\n",
       "      <td>226707</td>\n",
       "      <td>60</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Inch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034</td>\n",
       "      <td>39553978</td>\n",
       "      <td>18704</td>\n",
       "      <td>2180-07-23 12:36:00</td>\n",
       "      <td>2180-07-23 14:45:00</td>\n",
       "      <td>226730</td>\n",
       "      <td>152</td>\n",
       "      <td>152.0</td>\n",
       "      <td>cm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id   stay_id  caregiver_id            charttime  \\\n",
       "0    10000032  29079034  39553978         18704  2180-07-23 12:36:00   \n",
       "1    10000032  29079034  39553978         18704  2180-07-23 12:36:00   \n",
       "2    10000032  29079034  39553978         18704  2180-07-23 12:36:00   \n",
       "\n",
       "             storetime  itemid value  valuenum valueuom  warning  \n",
       "0  2180-07-23 14:45:00  226512  39.4      39.4       kg        0  \n",
       "1  2180-07-23 14:45:00  226707    60      60.0     Inch        0  \n",
       "2  2180-07-23 14:45:00  226730   152     152.0       cm        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- labevents -> /Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1/hosp/labevents.csv\n",
      "columns: ['labevent_id', 'subject_id', 'hadm_id', 'specimen_id', 'itemid', 'order_provider_id', 'charttime', 'storetime', 'value', 'valuenum', 'valueuom', 'ref_range_lower', 'ref_range_upper', 'flag', 'priority', 'comments']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labevent_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>specimen_id</th>\n",
       "      <th>itemid</th>\n",
       "      <th>order_provider_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>value</th>\n",
       "      <th>valuenum</th>\n",
       "      <th>valueuom</th>\n",
       "      <th>ref_range_lower</th>\n",
       "      <th>ref_range_upper</th>\n",
       "      <th>flag</th>\n",
       "      <th>priority</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2704548</td>\n",
       "      <td>50931</td>\n",
       "      <td>P69FQC</td>\n",
       "      <td>2180-03-23 11:51:00</td>\n",
       "      <td>2180-03-23 15:56:00</td>\n",
       "      <td>___</td>\n",
       "      <td>95.0</td>\n",
       "      <td>mg/dL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ROUTINE</td>\n",
       "      <td>IF FASTING, 70-100 NORMAL, &gt;125 PROVISIONAL DI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10000032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36092842</td>\n",
       "      <td>51071</td>\n",
       "      <td>P69FQC</td>\n",
       "      <td>2180-03-23 11:51:00</td>\n",
       "      <td>2180-03-23 16:00:00</td>\n",
       "      <td>NEG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ROUTINE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10000032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36092842</td>\n",
       "      <td>51074</td>\n",
       "      <td>P69FQC</td>\n",
       "      <td>2180-03-23 11:51:00</td>\n",
       "      <td>2180-03-23 16:00:00</td>\n",
       "      <td>NEG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ROUTINE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labevent_id  subject_id  hadm_id  specimen_id  itemid order_provider_id  \\\n",
       "0            1    10000032      NaN      2704548   50931            P69FQC   \n",
       "1            2    10000032      NaN     36092842   51071            P69FQC   \n",
       "2            3    10000032      NaN     36092842   51074            P69FQC   \n",
       "\n",
       "             charttime            storetime value  valuenum valueuom  \\\n",
       "0  2180-03-23 11:51:00  2180-03-23 15:56:00   ___      95.0    mg/dL   \n",
       "1  2180-03-23 11:51:00  2180-03-23 16:00:00   NEG       NaN      NaN   \n",
       "2  2180-03-23 11:51:00  2180-03-23 16:00:00   NEG       NaN      NaN   \n",
       "\n",
       "   ref_range_lower  ref_range_upper  flag priority  \\\n",
       "0             70.0            100.0   NaN  ROUTINE   \n",
       "1              NaN              NaN   NaN  ROUTINE   \n",
       "2              NaN              NaN   NaN  ROUTINE   \n",
       "\n",
       "                                            comments  \n",
       "0  IF FASTING, 70-100 NORMAL, >125 PROVISIONAL DI...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DATA_DIR and quick preview (copy of template)\n",
    "import os, pandas as pd\n",
    "DATA_DIR = '/Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1'\n",
    "print('DATA_DIR =', DATA_DIR)\n",
    "paths = {\n",
    "    'admissions': os.path.join(DATA_DIR, 'hosp', 'admissions.csv'),\n",
    "    'patients': os.path.join(DATA_DIR, 'hosp', 'patients.csv'),\n",
    "    'diagnoses_icd': os.path.join(DATA_DIR, 'hosp', 'diagnoses_icd.csv'),\n",
    "    'chartevents': os.path.join(DATA_DIR, 'icu', 'chartevents.csv'),\n",
    "    'labevents': os.path.join(DATA_DIR, 'hosp', 'labevents.csv'),\n",
    "}\n",
    "from IPython.display import display\n",
    "for name, p in paths.items():\n",
    "    print('---', name, '->', p)\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            df = pd.read_csv(p, nrows=5)\n",
    "            print('columns:', list(df.columns))\n",
    "            display(df.head(3))\n",
    "        except Exception as e:\n",
    "            print('读取失败:', e)\n",
    "    else:\n",
    "        print('NOT FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b49e21eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed readmit_30d, distribution:\n",
      "readmit_30d\n",
      "0    80.3\n",
      "1    19.7\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 30-day readmission label (copy of working code)\n",
    "import pandas as pd\n",
    "def _parse_dates_if_exists(df, cols):\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    return df\n",
    "admissions = pd.read_csv(os.path.join(DATA_DIR, 'hosp', 'admissions.csv'))\n",
    "admissions = _parse_dates_if_exists(admissions, ['admittime', 'dischtime', 'deathtime', 'edregtime', 'edouttime'])\n",
    "admissions = admissions.sort_values(['subject_id', 'admittime']).reset_index(drop=True)\n",
    "readmit_30d = []\n",
    "for idx, row in admissions.iterrows():\n",
    "    subject_id = row['subject_id']\n",
    "    hadm_id = row['hadm_id']\n",
    "    dischtime = row['dischtime']\n",
    "    if pd.isna(dischtime):\n",
    "        readmit_30d.append(0)\n",
    "        continue\n",
    "    future_admissions = admissions[\n",
    "        (admissions['subject_id'] == subject_id) &\n",
    "        (admissions['hadm_id'] != hadm_id) &\n",
    "        (admissions['admittime'] > dischtime)\n",
    "    ]\n",
    "    if not future_admissions.empty:\n",
    "        next_admit = future_admissions.iloc[0]['admittime']\n",
    "        days_to_readmit = (next_admit - dischtime).total_seconds() / (24*3600)\n",
    "        readmit_30d.append(1 if days_to_readmit <= 30 else 0)\n",
    "    else:\n",
    "        readmit_30d.append(0)\n",
    "admissions['readmit_30d'] = readmit_30d\n",
    "print('Computed readmit_30d, distribution:')\n",
    "print(admissions['readmit_30d'].value_counts(normalize=True).mul(100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd8ab264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load patients/diagnoses and time-series (with date parsing)\n",
    "def _read(path, **kwargs):\n",
    "    if os.path.exists(path):\n",
    "        return pd.read_csv(path, **kwargs)\n",
    "    raise FileNotFoundError(path)\n",
    "patients = _read(os.path.join(DATA_DIR, 'hosp', 'patients.csv'))\n",
    "diagnoses = _read(os.path.join(DATA_DIR, 'hosp', 'diagnoses_icd.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0583c8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_df sample:\n",
      "   subject_id   hadm_id  age  los_days  readmit_30d\n",
      "0    10000032  22595853   52  0.786111            0\n",
      "1    10000032  22841357   52  1.015278            1\n",
      "2    10000032  29079034   52  2.222222            1\n",
      "3    10000032  25742920   52  1.754167            0\n",
      "4    10000068  25022803   19  0.298611            0\n"
     ]
    }
   ],
   "source": [
    "# Static features and pooled time-series features (copy of functions)\n",
    "admissions = admissions.merge(patients[['subject_id','anchor_age','anchor_year']], on='subject_id', how='left')\n",
    "admissions['admit_year'] = admissions['admittime'].dt.year\n",
    "admissions['age'] = admissions['anchor_age'] + (admissions['admit_year'] - admissions['anchor_year'])\n",
    "admissions['age'] = admissions['age'].clip(lower=0)\n",
    "admissions['los_days'] = (admissions['dischtime'] - admissions['admittime']).dt.total_seconds()/(3600*24)\n",
    "static_df = admissions[['subject_id','hadm_id','age','los_days','readmit_30d']].drop_duplicates().reset_index(drop=True)\n",
    "print('static_df sample:')\n",
    "print(static_df.head())\n",
    "# For full time-series pooling we reference chartevents and labevents; those are large and may be filtered in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7581da",
   "metadata": {},
   "source": [
    "## Preprocessing summary\n",
    "- Parsed `admittime`/`dischtime`/`charttime` where present.\n",
    "- Constructed `readmit_30d` by checking the next admission within 30 days after discharge for the same `subject_id`.\n",
    "- Computed approximate `age` using `anchor_age` and `anchor_year`.\n",
    "- Built `static_df` with `age`, `los_days`, and the `readmit_30d` label.\n",
    "- Time-series pooling and imputation code (copied to template) produces `X_ts_imputed` and pooled summary features.\n",
    "\n",
    "Detailed reasoning for each preprocessing step (why I do it):\n",
    "\n",
    "- Date parsing (admittime/dischtime/charttime):\n",
    "  - Why: time calculations (length-of-stay, event windows, time-based labels) require proper datetime types. Parsing with `errors='coerce'` avoids crashes when a few malformed rows exist.\n",
    "  - Risk/caveat: coerce converts bad values to NaT; inspect parsing rates for unexpected drops.\n",
    "\n",
    "- Compute 30-day readmission label (`readmit_30d`):\n",
    "  - Why: the project's primary outcome is whether the patient returns within 30 days. Using the same `subject_id` and comparing `admittime` > `dischtime` is robust to overlapping stays and ordering.\n",
    "  - Choices made: we consider the *next* admission chronologically for that patient; alternative definitions (any admission within 30 days, excluding planned readmissions) can be implemented later.\n",
    "  - Caveat: planned readmissions and transfers should be excluded for some analyses; add filters if you have those flags.\n",
    "\n",
    "- Age estimation using `anchor_age` / `anchor_year`:\n",
    "  - Why: this MIMIC-IV export doesn't include `dob`, so `anchor_age` + (admit_year - anchor_year) gives an approximate age at admission.\n",
    "  - Caveat: this is approximate and may be off by a year or two due to anchor-year bucketing.\n",
    "\n",
    "- Static feature selection (age, gender, LOS):\n",
    "  - Why: these are baseline covariates known to correlate with readmission risk and are cheap to compute. Keep the set small for the smoke test; expand later.\n",
    "\n",
    "- Time-window selection and pooling (72-hour window, 1H resample):\n",
    "  - Why: using a fixed early-window (e.g., first 72 hours) standardizes inputs and focuses on early inpatient signals that could predict readmission risk. Hourly resampling balances temporal resolution and computational cost.\n",
    "  - Alternatives: longer windows or different pooling (min/max/last/percentiles) depending on the problem framing.\n",
    "\n",
    "- Imputation strategy (linear interpolation then mean fill):\n",
    "  - Why: clinical time series are irregular; interpolation fills short gaps while forward/backward filling and mean imputation handle longer missing stretches. This is a pragmatic choice for baseline models.\n",
    "  - Caveat: imputation can leak future info if misused (we only interpolate inside the admission window from observed times). For causal/time-to-event work, prefer models that handle irregular time or explicit missingness indicators.\n",
    "\n",
    "- Pooling into summary stats (mean, std, min, max):\n",
    "  - Why: reduces time-series to compact fixed-length vectors suitable for classical ML baselines (LogReg, RF) and quick experiments. These pooled features are interpretable and fast.\n",
    "  - Next step: use sequence models (LSTM/Transformer) on full time-series for richer temporal modeling.\n",
    "\n",
    "- Class weighting and stratified CV:\n",
    "  - Why: readmission is imbalanced (~20% positive in your preview). Using `class_weight='balanced'` and stratified splits makes training and evaluation more robust and comparable.\n",
    "\n",
    "Caveats & next improvements:\n",
    "- Add explicit cohort exclusions (planned readmissions, transfers, hospice, death during stay) depending on your study design.\n",
    "- Save parsing logs (rows parsed, NaT count) to detect data drift or broken exports.\n",
    "- Consider more advanced imputation (KNN, learned imputers) or models that explicitly handle missingness.\n",
    "- For reproducibility, pin the random seed and record data slices used for smoke tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70e7c91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting smoke test: time-series pooling + imputation + quick CV...\n",
      "Loading small slices of chartevents/labevents (this may still take a few seconds)...\n",
      "Built small dataset shapes: (34, 2) (34, 72, 9) (34,)\n",
      "Final pooled feature shape: (34, 38)\n",
      "Smoke test results:\n",
      "LR AUROC: 0.7222222222222223 AUPRC: 0.3666666666666667\n",
      "RF AUROC: 0.7777777777777778 AUPRC: 0.45\n",
      "Smoke test complete.\n",
      "Built small dataset shapes: (34, 2) (34, 72, 9) (34,)\n",
      "Final pooled feature shape: (34, 38)\n",
      "Smoke test results:\n",
      "LR AUROC: 0.7222222222222223 AUPRC: 0.3666666666666667\n",
      "RF AUROC: 0.7777777777777778 AUPRC: 0.45\n",
      "Smoke test complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v0/1lrkgf3s4d3_x4q14m3pmy840000gn/T/ipykernel_92993/2484787998.py:205: RuntimeWarning: Mean of empty slice\n",
      "  mean = np.nanmean(arr, axis=0)\n",
      "/opt/anaconda3/envs/mimic-py311/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:2053: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/v0/1lrkgf3s4d3_x4q14m3pmy840000gn/T/ipykernel_92993/2484787998.py:207: RuntimeWarning: All-NaN slice encountered\n",
      "  mn = np.nanmin(arr, axis=0)\n",
      "/var/folders/v0/1lrkgf3s4d3_x4q14m3pmy840000gn/T/ipykernel_92993/2484787998.py:208: RuntimeWarning: All-NaN slice encountered\n",
      "  mx = np.nanmax(arr, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# --------- Time-series pooling, imputation, and small smoke test (nrows) ---------\n",
    "print('Starting smoke test: time-series pooling + imputation + quick CV...')\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "\n",
    "# Default seed if not set earlier\n",
    "if 'RANDOM_SEED' not in globals():\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "# Ensure core dataframes exist; if not, load/compute them (makes this cell runnable standalone)\n",
    "if 'static_df' not in globals():\n",
    "    print('static_df not found in memory — constructing from admissions & patients (this may take a few seconds)')\n",
    "    def _parse_dates_if_exists(df, cols):\n",
    "        for col in cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        return df\n",
    "    def _read(path, **kwargs):\n",
    "        if os.path.exists(path):\n",
    "            return pd.read_csv(path, **kwargs)\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "    admissions = pd.read_csv(os.path.join(DATA_DIR, 'hosp', 'admissions.csv'))\n",
    "    admissions = _parse_dates_if_exists(admissions, ['admittime', 'dischtime', 'deathtime', 'edregtime', 'edouttime'])\n",
    "    admissions = admissions.sort_values(['subject_id', 'admittime']).reset_index(drop=True)\n",
    "\n",
    "    # compute readmit_30d if missing\n",
    "    if 'readmit_30d' not in admissions.columns:\n",
    "        readmit_30d = []\n",
    "        for idx, row in admissions.iterrows():\n",
    "            subject_id = row['subject_id']\n",
    "            hadm_id = row['hadm_id']\n",
    "            dischtime = row['dischtime']\n",
    "            if pd.isna(dischtime):\n",
    "                readmit_30d.append(0)\n",
    "                continue\n",
    "            future_admissions = admissions[\n",
    "                (admissions['subject_id'] == subject_id) &\n",
    "                (admissions['hadm_id'] != hadm_id) &\n",
    "                (admissions['admittime'] > dischtime)\n",
    "            ]\n",
    "            if not future_admissions.empty:\n",
    "                next_admit = future_admissions.iloc[0]['admittime']\n",
    "                days_to_readmit = (next_admit - dischtime).total_seconds() / (24*3600)\n",
    "                readmit_30d.append(1 if days_to_readmit <= 30 else 0)\n",
    "            else:\n",
    "                readmit_30d.append(0)\n",
    "        admissions['readmit_30d'] = readmit_30d\n",
    "    \n",
    "    # load patients and merge anchor_age/year\n",
    "    patients = _read(os.path.join(DATA_DIR, 'hosp', 'patients.csv'))\n",
    "    if 'anchor_age' in patients.columns and 'anchor_year' in patients.columns:\n",
    "        admissions = admissions.merge(patients[['subject_id','anchor_age','anchor_year']], on='subject_id', how='left')\n",
    "        admissions['admit_year'] = admissions['admittime'].dt.year\n",
    "        admissions['age'] = admissions['anchor_age'] + (admissions['admit_year'] - admissions['anchor_year'])\n",
    "        admissions['age'] = admissions['age'].clip(lower=0)\n",
    "    else:\n",
    "        # Fallback if anchor_age/year missing\n",
    "        admissions['age'] = np.nan\n",
    "\n",
    "    admissions['los_days'] = (admissions['dischtime'] - admissions['admittime']).dt.total_seconds()/(3600*24)\n",
    "    static_df = admissions[['subject_id','hadm_id','age','los_days','readmit_30d']].drop_duplicates().reset_index(drop=True)\n",
    "    print('Constructed static_df; sample:')\n",
    "    print(static_df.head())\n",
    "\n",
    "# Read small chunks of chartevents / labevents for smoke testing (adjust nrows as needed)\n",
    "ce_path = os.path.join(DATA_DIR, 'icu', 'chartevents.csv')\n",
    "le_path = os.path.join(DATA_DIR, 'hosp', 'labevents.csv')\n",
    "\n",
    "# Guard: if files absent, skip\n",
    "if not os.path.exists(ce_path) or not os.path.exists(le_path):\n",
    "    print('chartevents or labevents not found; skipping smoke test of time-series.')\n",
    "else:\n",
    "    print('Loading small slices of chartevents/labevents (this may still take a few seconds)...')\n",
    "    try:\n",
    "        ce = pd.read_csv(ce_path, usecols=['subject_id','hadm_id','charttime','itemid','valuenum','value'], nrows=200000, low_memory=False)\n",
    "    except Exception:\n",
    "        ce = pd.read_csv(ce_path, nrows=200000, low_memory=False)\n",
    "    try:\n",
    "        le = pd.read_csv(le_path, usecols=['labevent_id','subject_id','hadm_id','charttime','itemid','valuenum','value'], nrows=50000, low_memory=False)\n",
    "    except Exception:\n",
    "        le = pd.read_csv(le_path, nrows=50000, low_memory=False)\n",
    "\n",
    "    # use lowercase 'h' for resampling compatibility warnings\n",
    "    ce = _parse_dates_if_exists(ce, ['charttime'])\n",
    "    le = _parse_dates_if_exists(le, ['charttime'])\n",
    "\n",
    "    # Define simple item lists (same as template) — these are examples and can be customized\n",
    "    vital_items_of_interest = {\n",
    "        'heart_rate': [211, 220045],\n",
    "        'sys_bp': [220179, 51],\n",
    "        'dias_bp': [220180, 8368],\n",
    "        'resp_rate': [220210, 618],\n",
    "        'temp': [223761, 678],\n",
    "        'spo2': [220277]\n",
    "    }\n",
    "    lab_items_of_interest = {\n",
    "        'creatinine': [50912],\n",
    "        'glucose': [807, 823],\n",
    "        'wbc': [730],\n",
    "    }\n",
    "\n",
    "    def get_patient_events_smoke(hadm_id, window_hours=72, resample_freq='1h'):\n",
    "        # Use the small loaded slices (ce, le) — return None if no data\n",
    "        adm_row = admissions[admissions['hadm_id']==hadm_id]\n",
    "        if adm_row.empty:\n",
    "            return None\n",
    "        t0 = adm_row.iloc[0]['admittime']\n",
    "        t_end = t0 + pd.Timedelta(hours=window_hours)\n",
    "        ce_sub = ce[(ce['hadm_id']==hadm_id) & (ce['charttime']>=t0) & (ce['charttime']<=t_end)] if not ce.empty else pd.DataFrame()\n",
    "        le_sub = le[(le['hadm_id']==hadm_id) & (le['charttime']>=t0) & (le['charttime']<=t_end)] if not le.empty else pd.DataFrame()\n",
    "        rows = []\n",
    "        for var, itemids in vital_items_of_interest.items():\n",
    "            if ce_sub.empty:\n",
    "                continue\n",
    "            tmp = ce_sub[ce_sub['itemid'].isin(itemids)][['charttime','valuenum']].dropna()\n",
    "            if tmp.empty:\n",
    "                continue\n",
    "            tmp = tmp.rename(columns={'charttime':'time','valuenum':var})\n",
    "            tmp[var] = pd.to_numeric(tmp[var], errors='coerce')\n",
    "            rows.append(tmp.set_index('time')[var])\n",
    "        for var, itemids in lab_items_of_interest.items():\n",
    "            if le_sub.empty:\n",
    "                continue\n",
    "            tmp = le_sub[le_sub['itemid'].isin(itemids)][['charttime','valuenum']].dropna()\n",
    "            if tmp.empty:\n",
    "                continue\n",
    "            tmp = tmp.rename(columns={'charttime':'time','valuenum':var})\n",
    "            tmp[var] = pd.to_numeric(tmp[var], errors='coerce')\n",
    "            rows.append(tmp.set_index('time')[var])\n",
    "        if not rows:\n",
    "            return None\n",
    "        combined = pd.concat(rows, axis=1)\n",
    "        combined = combined.resample(resample_freq).mean()\n",
    "        combined = combined[:t0 + pd.Timedelta(hours=window_hours)]\n",
    "        return combined\n",
    "\n",
    "    # Build a small dataset for first N hadm_ids\n",
    "    hadm_ids_small = static_df['hadm_id'].dropna().unique()[:50]\n",
    "    X_static_small = []\n",
    "    X_ts_small = []\n",
    "    y_small = []\n",
    "    all_cols = list(vital_items_of_interest.keys()) + list(lab_items_of_interest.keys())\n",
    "\n",
    "    for hid in hadm_ids_small:\n",
    "        s = static_df[static_df['hadm_id']==hid]\n",
    "        if s.empty:\n",
    "            continue\n",
    "        ts = get_patient_events_smoke(hid, window_hours=72, resample_freq='1h')\n",
    "        if ts is None:\n",
    "            continue\n",
    "        for c in all_cols:\n",
    "            if c not in ts.columns:\n",
    "                ts[c] = np.nan\n",
    "        ts = ts[all_cols]\n",
    "        # ensure TIMESTEPS\n",
    "        if len(ts) < 72:\n",
    "            pad_len = 72 - len(ts)\n",
    "            if len(ts) == 0:\n",
    "                continue\n",
    "            pad_df = pd.DataFrame(np.nan, index=pd.date_range(ts.index[-1]+pd.Timedelta(hours=1), periods=pad_len, freq='1h'), columns=ts.columns)\n",
    "            ts = pd.concat([ts, pad_df])\n",
    "        else:\n",
    "            ts = ts.iloc[:72]\n",
    "        # Static selection: age and los_days (if los_days NaN, fill 0)\n",
    "        s_age = s['age'].iloc[0] if not pd.isna(s['age'].iloc[0]) else 0.0\n",
    "        s_los = s['los_days'].iloc[0] if not pd.isna(s['los_days'].iloc[0]) else 0.0\n",
    "        X_static_small.append(np.array([s_age, s_los], dtype=float))\n",
    "        X_ts_small.append(ts.values.astype(float))\n",
    "        y_small.append(int(s['readmit_30d'].iloc[0]))\n",
    "\n",
    "    X_static_small = np.array(X_static_small)\n",
    "    X_ts_small = np.array(X_ts_small)\n",
    "    y_small = np.array(y_small)\n",
    "\n",
    "    if len(y_small) == 0:\n",
    "        print('No time-series data available for the small sample with the chosen item lists. Consider increasing nrows or adjusting itemid lists.')\n",
    "    else:\n",
    "        print('Built small dataset shapes:', X_static_small.shape, X_ts_small.shape, y_small.shape)\n",
    "\n",
    "        # Impute time-series\n",
    "        def impute_time_series_array(X_ts):\n",
    "            N, T, F = X_ts.shape\n",
    "            X_imputed = X_ts.copy()\n",
    "            for i in range(N):\n",
    "                df_ts = pd.DataFrame(X_ts[i], columns=[f'f{j}' for j in range(F)])\n",
    "                df_ts = df_ts.interpolate(method='linear', limit_direction='both', axis=0).ffill().bfill()\n",
    "                df_ts = df_ts.fillna(df_ts.mean())\n",
    "                X_imputed[i] = df_ts.values\n",
    "            return X_imputed\n",
    "\n",
    "        X_ts_imputed_small = impute_time_series_array(X_ts_small)\n",
    "\n",
    "        # Pool time-series features\n",
    "        def pool_time_series_features(X_ts):\n",
    "            N, T, F = X_ts.shape\n",
    "            feats = []\n",
    "            for i in range(N):\n",
    "                arr = X_ts[i]\n",
    "                mean = np.nanmean(arr, axis=0)\n",
    "                std = np.nanstd(arr, axis=0)\n",
    "                mn = np.nanmin(arr, axis=0)\n",
    "                mx = np.nanmax(arr, axis=0)\n",
    "                # Handle all-NaN columns by replacing with zeros (safe for small baseline models)\n",
    "                mean = np.nan_to_num(mean, nan=0.0)\n",
    "                std = np.nan_to_num(std, nan=0.0)\n",
    "                mn = np.nan_to_num(mn, nan=0.0)\n",
    "                mx = np.nan_to_num(mx, nan=0.0)\n",
    "                feats.append(np.concatenate([mean, std, mn, mx]))\n",
    "            return np.array(feats)\n",
    "\n",
    "        X_pool_small = pool_time_series_features(X_ts_imputed_small)\n",
    "\n",
    "        # Scale static features\n",
    "        scaler_small = StandardScaler()\n",
    "        X_static_scaled_small = scaler_small.fit_transform(X_static_small)\n",
    "\n",
    "        X_final_small = np.hstack([X_static_scaled_small, X_pool_small])\n",
    "        print('Final pooled feature shape:', X_final_small.shape)\n",
    "\n",
    "        # Quick train/test split and evaluation\n",
    "        try:\n",
    "            if len(np.unique(y_small))>1:\n",
    "                Xtr, Xte, ytr, yte = train_test_split(X_final_small, y_small, test_size=0.3, random_state=RANDOM_SEED, stratify=y_small)\n",
    "            else:\n",
    "                Xtr, Xte, ytr, yte = train_test_split(X_final_small, y_small, test_size=0.3, random_state=RANDOM_SEED)\n",
    "        except Exception:\n",
    "            Xtr, Xte, ytr, yte = train_test_split(X_final_small, y_small, test_size=0.3, random_state=RANDOM_SEED)\n",
    "\n",
    "        lr = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, class_weight='balanced')\n",
    "\n",
    "        lr.fit(Xtr, ytr)\n",
    "        rf.fit(Xtr, ytr)\n",
    "\n",
    "        lr_p = lr.predict_proba(Xte)[:,1]\n",
    "        rf_p = rf.predict_proba(Xte)[:,1]\n",
    "\n",
    "        print('Smoke test results:')\n",
    "        if len(set(yte))>1:\n",
    "            print('LR AUROC:', roc_auc_score(yte, lr_p), 'AUPRC:', average_precision_score(yte, lr_p))\n",
    "            print('RF AUROC:', roc_auc_score(yte, rf_p), 'AUPRC:', average_precision_score(yte, rf_p))\n",
    "        else:\n",
    "            print('Insufficient label variation in test split to compute AUROC/AUPRC.')\n",
    "\n",
    "print('Smoke test complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running consolidated preprocessing and RNN (LSTM) baseline...\n"
     ]
    }
   ],
   "source": [
    "# Consolidated preprocessing + RNN baseline\n",
    "print('Running consolidated preprocessing and RNN (LSTM) baseline...')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Install TensorFlow if needed\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "except ImportError:\n",
    "    print('TensorFlow not installed. Installing...')\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'tensorflow'])\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Config - adjust for speed/memory\n",
    "NROWS_CE = 1000000  # Increased for more data\n",
    "NROWS_LE = 500000\n",
    "N_HADM = 5000  # Larger for \"full\" training\n",
    "WINDOW_HOURS = 72\n",
    "RESAMPLE_FREQ = '1h'\n",
    "RANDOM_SEED = globals().get('RANDOM_SEED', 42)\n",
    "TOP_K_VITALS = 5\n",
    "TOP_K_LABS = 4\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Paths\n",
    "ce_path = os.path.join(DATA_DIR, 'icu', 'chartevents.csv')\n",
    "le_path = os.path.join(DATA_DIR, 'hosp', 'labevents.csv')\n",
    "ads_path = os.path.join(DATA_DIR, 'hosp', 'admissions.csv')\n",
    "pts_path = os.path.join(DATA_DIR, 'hosp', 'patients.csv')\n",
    "\n",
    "# Read admissions and ensure label\n",
    "ads = pd.read_csv(ads_path, parse_dates=['admittime','dischtime'])\n",
    "if 'readmit_30d' not in ads.columns:\n",
    "    ads = ads.sort_values(['subject_id','admittime']).reset_index(drop=True)\n",
    "    ads['next_admit'] = ads.groupby('subject_id')['admittime'].shift(-1)\n",
    "    ads['days_to_next_admit'] = (ads['next_admit'] - ads['dischtime']).dt.total_seconds() / 86400.0\n",
    "    ads['readmit_30d'] = ((ads['days_to_next_admit'] >= 0) & (ads['days_to_next_admit'] <= 30)).astype(int)\n",
    "\n",
    "# Read slices\n",
    "if os.path.exists(ce_path):\n",
    "    try:\n",
    "        ce = pd.read_csv(ce_path, usecols=['subject_id','hadm_id','charttime','itemid','valuenum'], nrows=NROWS_CE, low_memory=False)\n",
    "    except Exception:\n",
    "        ce = pd.read_csv(ce_path, nrows=NROWS_CE, low_memory=False)\n",
    "    ce['charttime'] = pd.to_datetime(ce['charttime'], errors='coerce')\n",
    "else:\n",
    "    ce = pd.DataFrame()\n",
    "\n",
    "if os.path.exists(le_path):\n",
    "    try:\n",
    "        le = pd.read_csv(le_path, usecols=['subject_id','hadm_id','charttime','itemid','valuenum'], nrows=NROWS_LE, low_memory=False)\n",
    "    except Exception:\n",
    "        le = pd.read_csv(le_path, nrows=NROWS_LE, low_memory=False)\n",
    "    le['charttime'] = pd.to_datetime(le['charttime'], errors='coerce')\n",
    "else:\n",
    "    le = pd.DataFrame()\n",
    "\n",
    "# Top-K itemids\n",
    "if not ce.empty:\n",
    "    vital_counts = ce[ce['itemid'].notna()]['itemid'].value_counts()\n",
    "    vital_itemids = list(vital_counts.head(TOP_K_VITALS).index.astype(int))\n",
    "else:\n",
    "    vital_itemids = [220045, 220179, 220180, 220210, 220277]\n",
    "if not le.empty:\n",
    "    lab_counts = le[le['itemid'].notna()]['itemid'].value_counts()\n",
    "    lab_itemids = list(lab_counts.head(TOP_K_LABS).index.astype(int))\n",
    "else:\n",
    "    lab_itemids = [50912, 50813, 50882, 51267]\n",
    "\n",
    "canonical_itemids = sorted(set(vital_itemids + lab_itemids))\n",
    "F = len(canonical_itemids)  # Number of features\n",
    "\n",
    "def build_ts_pivot(hadm_id):\n",
    "    adm = ads[ads['hadm_id']==hadm_id]\n",
    "    if adm.empty:\n",
    "        return None\n",
    "    t0 = adm.iloc[0]['admittime']\n",
    "    t_end = t0 + pd.Timedelta(hours=WINDOW_HOURS)\n",
    "    rows = []\n",
    "    if not ce.empty:\n",
    "        sub = ce[(ce['hadm_id']==hadm_id) & (ce['charttime']>=t0) & (ce['charttime']<=t_end) & (ce['itemid'].isin(vital_itemids))]\n",
    "        if not sub.empty:\n",
    "            sub = sub.rename(columns={'charttime':'time','valuenum':'value'})\n",
    "            sub['hour'] = ((sub['time'] - t0).dt.total_seconds() // 3600).astype(int)\n",
    "            rows.append(sub.groupby(['hour','itemid'])['value'].mean().unstack(fill_value=np.nan))\n",
    "    if not le.empty:\n",
    "        sub = le[(le['hadm_id']==hadm_id) & (le['charttime']>=t0) & (le['charttime']<=t_end) & (le['itemid'].isin(lab_itemids))]\n",
    "        if not sub.empty:\n",
    "            sub = sub.rename(columns={'charttime':'time','valuenum':'value'})\n",
    "            sub['hour'] = ((sub['time'] - t0).dt.total_seconds() // 3600).astype(int)\n",
    "            rows.append(sub.groupby(['hour','itemid'])['value'].mean().unstack(fill_value=np.nan))\n",
    "    if not rows:\n",
    "        return None\n",
    "    pivot = pd.concat(rows, axis=1)\n",
    "    pivot = pivot.reindex(columns=canonical_itemids)\n",
    "    pivot = pivot.reindex(range(0, WINDOW_HOURS))\n",
    "    return pivot\n",
    "\n",
    "def impute_ts(pivot):\n",
    "    if pivot is None or pivot.shape[0]==0:\n",
    "        return None\n",
    "    arr = pivot.apply(pd.to_numeric, errors='coerce')\n",
    "    arr = arr.interpolate(limit_direction='both', axis=0).ffill().bfill()\n",
    "    arr = arr.fillna(arr.mean())\n",
    "    return np.nan_to_num(arr.values, nan=0.0)  # Ensure no NaN\n",
    "\n",
    "# Build dataset\n",
    "hadm_pool = ads['hadm_id'].dropna().unique()[:N_HADM]\n",
    "X_ts = []\n",
    "X_static = []\n",
    "y = []\n",
    "for hid in hadm_pool:\n",
    "    pivot = build_ts_pivot(hid)\n",
    "    ts = impute_ts(pivot)\n",
    "    if ts is None:\n",
    "        continue\n",
    "    row = ads[ads['hadm_id']==hid]\n",
    "    if row.empty:\n",
    "        continue\n",
    "    age = 0.0\n",
    "    los = 0.0\n",
    "    if 'anchor_age' in row.columns:\n",
    "        age = float(row['anchor_age'].iloc[0])\n",
    "    if pd.notna(row['dischtime'].iloc[0]):\n",
    "        los = (row['dischtime'].iloc[0] - row['admittime'].iloc[0]).total_seconds() / (3600*24)\n",
    "    X_ts.append(ts)\n",
    "    X_static.append([age, los])\n",
    "    y.append(int(row['readmit_30d'].iloc[0]))\n",
    "\n",
    "if len(X_ts) < 10:\n",
    "    print('Too few samples built (', len(X_ts), ') — try increasing NROWS or N_HADM')\n",
    "else:\n",
    "    X_ts = np.array(X_ts)  # (N, 72, F)\n",
    "    X_static = np.array(X_static)  # (N, 2)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Scale static\n",
    "    scaler = StandardScaler()\n",
    "    X_static_scaled = scaler.fit_transform(X_static)\n",
    "    \n",
    "    # Repeat static for each timestep (simple way to include)\n",
    "    X_static_repeated = np.repeat(X_static_scaled[:, np.newaxis, :], WINDOW_HOURS, axis=1)  # (N, 72, 2)\n",
    "    X_combined = np.concatenate([X_ts, X_static_repeated], axis=2)  # (N, 72, F+2)\n",
    "    \n",
    "    print(f'Built dataset: {X_combined.shape[0]} samples, {X_combined.shape[1]} timesteps, {X_combined.shape[2]} features')\n",
    "    \n",
    "    # Class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    \n",
    "    # RNN Model\n",
    "    def build_rnn_model(input_shape):\n",
    "        model = Sequential([\n",
    "            LSTM(64, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    # CV\n",
    "    if len(np.unique(y)) > 1 and len(y) >= 5:\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "        aucs = []\n",
    "        auprcs = []\n",
    "        for fold, (tr_idx, te_idx) in enumerate(skf.split(X_combined, y)):\n",
    "            print(f'Training fold {fold+1}/5...')\n",
    "            Xtr, Xte = X_combined[tr_idx], X_combined[te_idx]\n",
    "            ytr, yte = y[tr_idx], y[te_idx]\n",
    "            \n",
    "            model = build_rnn_model((WINDOW_HOURS, X_combined.shape[2]))\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "            model.fit(Xtr, ytr, epochs=50, batch_size=32, validation_split=0.2, class_weight=class_weight_dict, callbacks=[early_stop], verbose=0)\n",
    "            \n",
    "            probs = model.predict(Xte).flatten()\n",
    "            if np.isnan(probs).any():\n",
    "                print(f'Warning: NaN in predictions for fold {fold+1}, skipping.')\n",
    "                continue\n",
    "            aucs.append(roc_auc_score(yte, probs))\n",
    "            auprcs.append(average_precision_score(yte, probs))\n",
    "        \n",
    "        print('Stratified 5-fold CV (RNN LSTM)')\n",
    "        print(f'AUROC: {np.mean(aucs):.4f} ± {np.std(aucs):.4f}')\n",
    "        print(f'AUPRC: {np.mean(auprcs):.4f} ± {np.std(auprcs):.4f}')\n",
    "        auroc_mean = float(np.mean(aucs))\n",
    "    else:\n",
    "        print('Insufficient data for CV; using single split.')\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        Xtr, Xte, ytr, yte = train_test_split(X_combined, y, test_size=0.3, random_state=RANDOM_SEED, stratify=y)\n",
    "        model = build_rnn_model((WINDOW_HOURS, X_combined.shape[2]))\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        model.fit(Xtr, ytr, epochs=50, batch_size=32, validation_split=0.2, class_weight=class_weight_dict, callbacks=[early_stop], verbose=0)\n",
    "        probs = model.predict(Xte).flatten()\n",
    "        if len(set(yte)) > 1:\n",
    "            print('Single-split RNN AUROC:', roc_auc_score(yte, probs))\n",
    "            print('Single-split RNN AUPRC:', average_precision_score(yte, probs))\n",
    "            auroc_mean = float(roc_auc_score(yte, probs))\n",
    "        else:\n",
    "            print('No AUROC/AUPRC.')\n",
    "            auroc_mean = None\n",
    "\n",
    "# Save summary\n",
    "out = {\n",
    "    'n_samples': int(len(X_ts)) if 'X_ts' in locals() else 0,\n",
    "    'n_timesteps': WINDOW_HOURS,\n",
    "    'n_features': X_combined.shape[2] if 'X_combined' in locals() else 0,\n",
    "    'auroc': auroc_mean\n",
    "}\n",
    "out_path = os.path.join('yuchen_testing', 'rnn_baseline_summary.json')\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "with open(out_path, 'w') as f:\n",
    "    import json\n",
    "    json.dump(out, f)\n",
    "\n",
    "print('Done. Summary written to', out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
