{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac7c278e",
   "metadata": {},
   "source": [
    "# Expanded preprocessing + smoke test\n",
    "\n",
    "This notebook prepares a slightly larger smoke test for the 30-day readmission preprocessing pipeline.\n",
    "Change the NROWS and N_HADM variables in Cell 2 to control how much data is loaded for the smoke test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a66301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config: adjust these for local runs\n",
    "DATA_DIR = '/Users/yuchenzhou/documents/duke/compsci526/final_proj/mimic-iv-3.1'\n",
    "# How many chartevents / labevents rows to load (set low for quick smoke-test)\n",
    "NROWS_CHARTEVENTS = 500000\n",
    "NROWS_LABEVENTS = 200000\n",
    "# How many hadm_ids to attempt to build (aim for ~100-300 for larger smoke-test)\n",
    "N_HADM = 200\n",
    "# random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "# First hours to keep from admission\n",
    "FIRST_HOURS = 72\n",
    "# Hourly resampling frequency\n",
    "RESAMPLE_FREQ = '1H'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ca079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, math, random, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fabbb9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: safe read with parse dates if present\n",
    "def _read(path, nrows=None):\n",
    "    parse_dates = []\n",
    "    for col in ['admittime','dischtime','deathtime','charttime','charttime','charttime','chartdate','charttime_ts','charttime_local']:\n",
    "        # presence-checked possible date columns; pandas will ignore unknown\n",
    "        if col in pd.read_csv(path, nrows=0).columns:\n",
    "            parse_dates.append(col)\n",
    "    return pd.read_csv(path, parse_dates=parse_dates or None, nrows=nrows)\n",
    "\n",
    "def compute_readmit_30d(adm_df):\n",
    "    # expects admissions with subject_id, hadm_id, admit_dt and discharge_dt already parsed\n",
    "    df = adm_df.sort_values(['subject_id','admittime'])[[ 'subject_id','hadm_id','admittime','dischtime' ]].copy()\n",
    "    df['next_admit'] = df.groupby('subject_id')['admittime'].shift(-1)\n",
    "    df['days_to_next_admit'] = (df['next_admit'] - df['dischtime']).dt.total_seconds() / 86400.0\n",
    "    df['readmit_30d'] = (df['days_to_next_admit'] >= 0) & (df['days_to_next_admit'] <= 30)\n",
    "    return df[['subject_id','hadm_id','readmit_30d']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314225fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading admissions (header check)...\n",
      "Admissions rows: 546028\n",
      "Labels computed, positive rate: 0.20025529826309274\n"
     ]
    }
   ],
   "source": [
    "# Top-level load of admissions & patients (small) to compute labels and select hadm_ids\n",
    "ads_path = os.path.join(DATA_DIR, 'hosp', 'admissions.csv')\n",
    "pts_path = os.path.join(DATA_DIR, 'hosp', 'patients.csv')\n",
    "print('Reading admissions (header check)...')\n",
    "admissions = pd.read_csv(ads_path, parse_dates=['admittime','dischtime'], nrows=None)\n",
    "patients = pd.read_csv(pts_path, parse_dates=['anchor_year']) if os.path.exists(pts_path) else pd.DataFrame()\n",
    "labels_df = compute_readmit_30d(admissions)\n",
    "print('Admissions rows:', len(admissions))\n",
    "print('Labels computed, positive rate:', labels_df['readmit_30d'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c6ee7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vitals and labs lists lengths: 7 8\n"
     ]
    }
   ],
   "source": [
    "# Expanded itemid lists (examples; adjust as needed)\n",
    "vital_items_of_interest = [220045, 220210, 220277, 220045, 220046, 220047, 220048]\n",
    "lab_items_of_interest = [50820, 51464, 50931, 50912, 50813, 50882, 50893, 51267]\n",
    "print('vitals and labs lists lengths:', len(vital_items_of_interest), len(lab_items_of_interest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d721000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chartevents nrows= 500000\n",
      "Read chartevents rows: 500000\n",
      "Reading labevents nrows= 200000\n",
      "Read labevents rows: 200000\n"
     ]
    }
   ],
   "source": [
    "# Load slices of chartevents and labevents for the smoke test (may take memory)\n",
    "ce_path = os.path.join(DATA_DIR, 'icu', 'chartevents.csv')\n",
    "le_path = os.path.join(DATA_DIR, 'hosp', 'labevents.csv')\n",
    "print('Reading chartevents nrows=', NROWS_CHARTEVENTS)\n",
    "chartevents = _read(ce_path, nrows=NROWS_CHARTEVENTS)\n",
    "print('Read chartevents rows:', len(chartevents))\n",
    "print('Reading labevents nrows=', NROWS_LABEVENTS)\n",
    "labevents = _read(le_path, nrows=NROWS_LABEVENTS)\n",
    "print('Read labevents rows:', len(labevents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06cb8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow events to items of interest and create a unified time column\n",
    "def prepare_events(df, time_col_candidates=('charttime','chartdate','charttime_ts')):\n",
    "    # pick first available time col\n",
    "    time_col = next((c for c in time_col_candidates if c in df.columns), None)\n",
    "    if time_col is None:\n",
    "        raise ValueError('No time column found in events')\n",
    "    df = df.copy()\n",
    "    df['event_time'] = pd.to_datetime(df[time_col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "111dbec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared events shapes: (500000, 5) (100858, 5)\n"
     ]
    }
   ],
   "source": [
    "chartevents = prepare_events(chartevents)\n",
    "labevents = prepare_events(labevents)\n",
    "# reduce columns to speed later ops\n",
    "chartevents = chartevents[['subject_id','hadm_id','itemid','value','event_time']].dropna(subset=['hadm_id'])\n",
    "labevents = labevents[['subject_id','hadm_id','itemid','value','event_time']].dropna(subset=['hadm_id'])\n",
    "print('Prepared events shapes:', chartevents.shape, labevents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e5b6885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected hadm count: 200\n"
     ]
    }
   ],
   "source": [
    "# Candidate hadm pool: admissions with labels; sample up to N_HADM balanced by label\n",
    "pool = labels_df.merge(admissions[['hadm_id','subject_id','admittime','dischtime']], on=['subject_id','hadm_id'])\n",
    "pos = pool[pool['readmit_30d']].sample(frac=1.0, random_state=RANDOM_STATE)\n",
    "neg = pool[~pool['readmit_30d']].sample(frac=1.0, random_state=RANDOM_STATE)\n",
    "n_each = max(1, N_HADM // 2)\n",
    "sel = pd.concat([pos.head(n_each), neg.head(n_each)])\n",
    "print('Selected hadm count:', len(sel))\n",
    "hadm_ids = sel['hadm_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffcc57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset: for each hadm, extract first FIRST_HOURS of events, resample hourly and pool\n",
    "def build_patient_series(hadm_id):\n",
    "    adm = admissions[admissions['hadm_id'] == hadm_id].iloc[0]\n",
    "    sub_ce = chartevents[chartevents['hadm_id'] == hadm_id]\n",
    "    sub_le = labevents[labevents['hadm_id'] == hadm_id]\n",
    "    # combine and filter to FIRST_HOURS\n",
    "    start = adm['admittime']\n",
    "    end = adm['admittime'] + pd.Timedelta(hours=FIRST_HOURS)\n",
    "    ev = pd.concat([sub_ce, sub_le], ignore_index=True)\n",
    "    ev = ev[(ev['event_time'] >= start) & (ev['event_time'] <= end)]\n",
    "    if ev.empty:\n",
    "        return None\n",
    "    # pivot by hour and itemid using mean aggregation\n",
    "    ev = ev.assign(hour=lambda d: ((d['event_time'] - start).dt.total_seconds() // 3600).astype(int))\n",
    "    pivot = ev.groupby(['hour','itemid'])['value'].agg('mean').unstack(fill_value=np.nan)\n",
    "    # resample to ensure FIRST_HOURS rows\n",
    "    pivot = pivot.reindex(range(0, FIRST_HOURS))\n",
    "    return pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "287964de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built dataset rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Pooling & imputation: linear interp + ffill/bfill + column mean\n",
    "def impute_and_pool(df_hourly):\n",
    "    if df_hourly is None or df_hourly.shape[0] == 0:\n",
    "        return None\n",
    "    arr = df_hourly.copy()\n",
    "    arr = arr.apply(pd.to_numeric, errors='coerce')\n",
    "    arr = arr.interpolate(limit_direction='both', axis=0).ffill().bfill()\n",
    "    arr = arr.fillna(arr.mean())\n",
    "    pooled = []\n",
    "    pooled.extend(arr.mean(axis=0).values.tolist())\n",
    "    pooled.extend(arr.std(axis=0).values.tolist())\n",
    "    pooled.extend(arr.min(axis=0).values.tolist())\n",
    "    pooled.extend(arr.max(axis=0).values.tolist())\n",
    "    return np.array(pooled)\n",
    "\n",
    "# Build X, y\n",
    "X = []\n",
    "y = []\n",
    "hadm_with_features = []\n",
    "for hid in hadm_ids:\n",
    "    s = build_patient_series(hid)\n",
    "    vec = impute_and_pool(s)\n",
    "    if vec is not None and not np.any(np.isnan(vec)):\n",
    "        X.append(vec)\n",
    "        y.append(labels_df[labels_df['hadm_id']==hid]['readmit_30d'].iloc[0])\n",
    "        hadm_with_features.append(hid)\n",
    "print('Built dataset rows:', len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "668a82aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough rows to run baseline; need >=10\n"
     ]
    }
   ],
   "source": [
    "# Quick baseline evaluation (train/test split)\n",
    "if len(X) >= 10:\n",
    "    X_arr = np.vstack(X)\n",
    "    y_arr = np.array(y).astype(int)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_arr, y_arr, test_size=0.3, random_state=RANDOM_STATE, stratify=y_arr)\n",
    "    clf = LogisticRegression(class_weight='balanced', max_iter=500)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    probs = clf.predict_proba(Xte)[:,1]\n",
    "    print('LogReg AUROC:', roc_auc_score(yte, probs))\n",
    "    print('LogReg AUPRC:', average_precision_score(yte, probs))\n",
    "else:\n",
    "    print('Not enough rows to run baseline; need >=10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7f1a1",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- This notebook is a smoke test and may be memory heavy. Reduce NROWS_* and N_HADM for quick runs.\n",
    "- The pipeline uses simple pooling (mean/std/min/max) â€” good for classical baselines. For deep models keep full time-series.\n",
    "- If you want me to add StratifiedKFold CV and artifact saving, I can next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
