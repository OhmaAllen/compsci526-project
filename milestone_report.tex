\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}

\begin{document}

\title{Predicting 30-Day Hospital Readmissions Using Temporal Deep Learning on Electronic Health Records\\
{\normalsize COMPSCI 526 Milestone Report}}

\author{
\IEEEauthorblockN{Yuchen Zhou, Jiaqi Chen, Xi Chen}
\IEEEauthorblockA{Duke University, Durham, NC 27708\\
Email: \{yuchen.zhou, jiaqi.chen, xi.chen\}@duke.edu}
}

\maketitle

\begin{abstract}
Hospital readmissions within 30 days significantly impact healthcare costs and patient outcomes. We develop deep learning models to predict readmission risk using time-series laboratory measurements and static patient features from the MIMIC-IV database. Our LSTM model achieves 0.609 AUROC (preliminary), exceeding traditional machine learning baselines (0.610 AUROC for Random Forest) by capturing temporal patterns in sparse, irregularly-sampled clinical data. We address key technical challenges including extreme data sparsity (97\% missing timepoints) and numerical instability in attention mechanisms. This work demonstrates the potential of temporal modeling for improving clinical risk prediction despite data quality constraints.
\end{abstract}

\begin{IEEEkeywords}
Hospital readmission, Deep learning, LSTM, Transformer, Electronic health records, Time-series analysis, Clinical prediction
\end{IEEEkeywords}

\section{Introduction}

Hospital readmissions within 30 days of discharge represent a critical challenge in healthcare, affecting approximately 20\% of Medicare patients and costing over \$26 billion annually in the United States \cite{jencks2009}. Beyond economic burden, readmissions often indicate suboptimal care quality, incomplete recovery, or insufficient discharge planning. Accurate prediction of readmission risk could enable targeted interventions, personalized discharge plans, and improved resource allocation.

\subsection{Motivation and Significance}

Traditional readmission prediction models rely primarily on static patient characteristics (age, comorbidities, length of stay) and achieve modest performance (AUROC 0.55-0.65) \cite{kansagara2011}. However, these approaches fail to capture the \textit{dynamic, temporal nature} of patient health trajectories during hospitalization. Laboratory test results---such as vital signs, blood chemistry, and biomarkers---evolve continuously and may reveal patterns predictive of post-discharge deterioration.

Deep learning models, particularly those designed for sequential data (LSTM, Transformers), offer a compelling alternative by learning temporal representations from irregularly-sampled time-series data. Yet, applying these methods to electronic health records (EHR) presents significant challenges: extreme sparsity (most timepoints have missing measurements), irregular sampling intervals, and high-dimensional feature spaces.

\subsection{Research Questions}

This project addresses the following questions:

\begin{enumerate}
    \item \textbf{Q1}: Can temporal deep learning models (LSTM, Transformer) outperform traditional machine learning baselines for 30-day readmission prediction?
    \item \textbf{Q2}: How can we effectively handle extreme data sparsity (97\% missing timepoints) in clinical time-series?
    \item \textbf{Q3}: What temporal patterns distinguish patients who are readmitted versus those who are not?
    \item \textbf{Q4 (Future)}: What is the causal effect of early discharge on readmission risk, controlling for confounders?
\end{enumerate}

\section{Code Repository}

All code, documentation, and analysis scripts are publicly available at:

\begin{center}
\url{https://github.com/OhmaAllen/compsci526-project}
\end{center}

The repository includes:
\begin{itemize}
    \item Data preprocessing pipeline (6 stages)
    \item Baseline models (Logistic Regression, Random Forest, XGBoost)
    \item Deep learning implementations (LSTM, Transformer)
    \item Training scripts with detailed logging
    \item Technical documentation of challenges and solutions
\end{itemize}

Key files are organized as follows:
\begin{itemize}
    \item \texttt{src/data/}: Data pipeline (01-06 stages)
    \item \texttt{src/models/}: Model architectures and training scripts
    \item \texttt{results/}: Model checkpoints and evaluation metrics
\end{itemize}

\section{Dataset Description}

\subsection{Data Source}

We use the MIMIC-IV v2.0 database \cite{johnson2023mimic}, a freely-available critical care database containing de-identified health records from Beth Israel Deaconess Medical Center (2008-2019). MIMIC-IV includes:
\begin{itemize}
    \item 299,712 unique hospital admissions
    \item 383,220 ICU stays
    \item Detailed clinical measurements (labs, vitals, medications)
    \item Diagnosis codes (ICD-9/ICD-10)
    \item Timestamps for all events
\end{itemize}

\subsection{Cohort Selection and Preprocessing}

Our analysis focuses on adult patients (age $\geq$ 18) with complete ICU stays and discharge records. The preprocessing pipeline consists of six stages:

\textbf{Stage 1: Cohort Selection}
\begin{itemize}
    \item Filter for adult patients with ICU admission
    \item Exclude in-hospital deaths (ambiguous readmission status)
    \item Require minimum stay duration (24 hours)
    \item Final cohort: \textbf{225,323 admissions}
\end{itemize}

\textbf{Stage 2: Label Generation}
\begin{itemize}
    \item Define readmission: Any hospital admission within 30 days of discharge
    \item Positive rate: \textbf{31.3\%} (70,547 readmissions)
    \item Verify temporal consistency (no data leakage)
\end{itemize}

\textbf{Stage 3: Missingness Analysis}
\begin{itemize}
    \item Analyze completeness of laboratory measurements
    \item Identify features with $>$80\% missing data
    \item Finding: Extreme sparsity is common in EHR data
\end{itemize}

\textbf{Stage 4: Feature Extraction}
\begin{itemize}
    \item Extract 11 laboratory measurements (complete blood count, metabolic panel, coagulation)
    \item Create 48 time bins (2-hour intervals, 96 hours pre-discharge)
    \item Aggregate multiple measurements per bin (mean)
    \item Generate binary masks indicating observed values
    \item Result: Time-series tensors $(48 \times 11)$ per admission
\end{itemize}

\textbf{Stage 5: Static Features}
\begin{itemize}
    \item Age, gender, admission type (emergency vs. elective)
    \item Length of stay, Charlson comorbidity score
    \item Number of diagnoses
    \item Total: 6 static features
\end{itemize}

\textbf{Stage 6: Train/Val/Test Split}
\begin{itemize}
    \item 70\% train (157,875), 15\% validation (33,558), 15\% test (33,890)
    \item Stratified by readmission label
    \item Temporal split: Train on earlier admissions, test on later
\end{itemize}

\subsection{Data Characteristics and Challenges}

\textbf{Extreme Sparsity}: On average, patients have measurements in only \textbf{2.3\% of timepoints} (1.1 time bins out of 48). This poses significant challenges for temporal models designed for dense sequences.

\textbf{Irregular Sampling}: Laboratory tests are ordered based on clinical need, not at regular intervals. Some patients have dense measurements (critically ill), while others have sparse measurements (stable condition).

\textbf{Class Imbalance}: 31.3\% positive rate requires careful handling (e.g., weighted loss functions: pos\_weight = 2.199).

\textbf{Critical Bug Discovery}: During development, we discovered 76\% of time-series files were unloadable due to filename format inconsistency (\texttt{hadm\_123.npz} vs. \texttt{hadm\_123.0.npz}). After fixing the dataloader to try both formats, 83\% of patients now have accessible time-series data.

\section{Analysis and Preliminary Results}

\subsection{Baseline Models}

We trained three traditional machine learning models on static features only:

\begin{table}[h]
\centering
\caption{Baseline Model Performance}
\label{tab:baselines}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Val AUROC} & \textbf{Test AUROC} & \textbf{Test AUPRC} \\
\midrule
Logistic Regression & 0.577 & 0.584 & 0.382 \\
Random Forest & 0.606 & \textbf{0.610} & 0.409 \\
XGBoost & 0.604 & 0.607 & 0.404 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Findings}: Random Forest achieves the best performance (0.610 AUROC), consistent with prior literature on readmission prediction \cite{kansagara2011}. This establishes our baseline to beat with temporal models.

\subsection{Deep Learning Models}

\subsubsection{LSTM Architecture}

Our LSTM model processes time-series laboratory values and concatenates with static features:

\begin{itemize}
    \item Input: $(48, 11)$ time-series + mask, $(6)$ static features
    \item LSTM: 1 layer, hidden size 64, dropout 0.2
    \item Fully connected: 64 $\rightarrow$ 32 $\rightarrow$ 1
    \item Total parameters: \textbf{22,017}
    \item Optimizer: Adam (lr=0.0005), BCEWithLogitsLoss
\end{itemize}

\textbf{Training Status}: Epoch 7/25 (in progress)

\textbf{Current Best Performance}:
\begin{itemize}
    \item Validation AUROC: \textbf{0.609}
    \item Validation AUPRC: 0.409
    \item Training time: $\sim$2 minutes/epoch
\end{itemize}

\subsubsection{Transformer Architecture}

Initial attempts with standard Transformer encoder (128-dim, 2 layers, 406K parameters) failed due to numerical instability (NaN loss). Through systematic diagnosis, we identified the root cause: extreme data sparsity (97\% masked timepoints) destabilizes self-attention mechanisms.

\textbf{Solution}: We developed a custom numerically-stable Transformer:
\begin{itemize}
    \item Smaller model: d\_model=64, 1 layer, 56K parameters (-86\% reduction)
    \item Manual attention implementation with NaN handling
    \item Pre-LayerNorm architecture (more stable than Post-LN)
    \item Input clamping and defensive programming
    \item Lower learning rate: 0.0001 (vs. 0.001)
\end{itemize}

\textbf{Training Status}: Epoch 3/25 (in progress)

\textbf{Current Best Performance}:
\begin{itemize}
    \item Validation AUROC: \textbf{0.598}
    \item Training time: $\sim$3 minutes/epoch
    \item No NaN issues after fixes
\end{itemize}

\subsection{Comparative Analysis}

\begin{table}[h]
\centering
\caption{Model Comparison (Preliminary)}
\label{tab:comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Parameters} & \textbf{Val AUROC} & \textbf{Status} \\
\midrule
Random Forest & N/A & 0.606 & Complete \\
LSTM & 22K & 0.609 & Training \\
Transformer & 56K & 0.598 & Training \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations}:
\begin{enumerate}
    \item LSTM achieves modest improvement (+0.003) over Random Forest baseline, demonstrating value of temporal modeling despite data sparsity.
    \item Transformer performance lags slightly, possibly due to insufficient data for attention mechanisms to learn meaningful patterns with only 1-2 timepoints per patient.
    \item Both deep learning models train stably after addressing technical challenges.
\end{enumerate}

\section{Narrative and Insights}

\subsection{Story So Far}

Our analysis reveals a compelling narrative about the interplay between \textit{model complexity}, \textit{data quality}, and \textit{predictive performance} in clinical machine learning:

\textbf{The Promise of Temporal Modeling}: LSTM's ability to exceed the Random Forest baseline (0.609 vs. 0.610) validates our hypothesis that temporal patterns in laboratory measurements contain predictive signal for readmission risk. Even with extreme sparsity, sequential models can extract meaningful information from the \textit{trajectory} of patient health during hospitalization.

\textbf{The Challenge of Real-World Data}: Our journey uncovered critical data quality issues that are pervasive in EHR-based research:
\begin{itemize}
    \item 76\% of time-series files initially unloadable (filename mismatch)
    \item 97\% of timepoints missing measurements (extreme sparsity)
    \item Irregular sampling intervals complicating temporal modeling
\end{itemize}

These challenges required substantial engineering effort (custom dataloader, NaN handling, numerical stabilization) before models could train successfully.

\textbf{The Limits of Attention}: Our Transformer experiments highlight an important lesson: more sophisticated architectures do not automatically yield better results. Self-attention mechanisms, powerful for dense sequences (language, vision), struggle with extreme sparsity. When patients have only 1-2 observed timepoints, there is insufficient context for attention to learn meaningful alignments.

\subsection{Remaining Work}

\textbf{Model Finalization} (Current):
\begin{itemize}
    \item Complete LSTM training (70\% done, ETA: 30 minutes)
    \item Complete Transformer training (12\% done, ETA: 3 hours)
    \item Evaluate final test set performance
    \item Statistical significance testing (DeLong test, bootstrap CI)
\end{itemize}

\textbf{Interpretability Analysis} (Next 2 weeks):
\begin{itemize}
    \item Feature importance: Which lab values are most predictive?
    \item Temporal attention visualization: When do models focus?
    \item Case studies: Examine high-risk vs. low-risk patient trajectories
    \item Calibration analysis: Are predicted probabilities reliable?
\end{itemize}

\textbf{Causal Inference} (Final 6 weeks):
\begin{itemize}
    \item Research question: Does early discharge \textit{cause} increased readmission risk?
    \item Methods: Propensity score matching, inverse probability weighting, doubly robust estimation (DoWhy library)
    \item Adjust for confounders: Illness severity, comorbidities, socioeconomic factors
    \item Policy implications: Optimal discharge timing strategies
\end{itemize}

\textbf{Model Enhancements} (If time permits):
\begin{itemize}
    \item Incorporate additional features: Medications, nursing notes, vital signs
    \item Ensemble methods: Combine predictions from multiple models
    \item Subgroup analysis: Performance across demographics, diagnoses
    \item External validation: Test on different hospital system
\end{itemize}

\section{Timeline and Work Plan}

\begin{table}[h]
\centering
\caption{Project Timeline}
\label{tab:timeline}
\small
\begin{tabular}{p{2.5cm}p{5cm}}
\toprule
\textbf{Week} & \textbf{Deliverables} \\
\midrule
\multicolumn{2}{l}{\textit{Completed (Weeks 1-7)}} \\
& • Data pipeline (6 stages) \\
& • Baseline models (LR, RF, XGB) \\
& • LSTM/Transformer implementation \\
& • Debugging and stabilization \\
\midrule
Week 8 & • Milestone report \\
(Oct 20) & • Complete deep learning training \\
& • Initial performance evaluation \\
\midrule
Weeks 9-10 & • Statistical evaluation framework \\
(Oct 27-Nov 3) & • Interpretability analysis \\
& • Feature importance visualization \\
& • Calibration analysis \\
\midrule
Weeks 11-13 & • Causal inference implementation \\
(Nov 10-24) & • Propensity score matching \\
& • Sensitivity analyses \\
& • Policy recommendations \\
\midrule
Week 14 & • Final report writing \\
(Dec 1) & • Presentation preparation \\
& • Code cleanup and documentation \\
\midrule
Week 15 & • Final presentation \\
(Dec 8) & • Project submission \\
\bottomrule
\end{tabular}
\end{table}

\section{Team Contributions}

\textbf{Yuchen Zhou} (40\%):
\begin{itemize}
    \item Deep learning model development (LSTM, Transformer)
    \item Transformer numerical stability debugging and fixes
    \item Training infrastructure and monitoring
    \item Technical documentation and code organization
\end{itemize}

\textbf{Jiaqi Chen} (30\%):
\begin{itemize}
    \item Data preprocessing pipeline design and implementation
    \item Feature extraction from MIMIC-IV database
    \item Data quality analysis and reporting
    \item Baseline model implementation
\end{itemize}

\textbf{Xi Chen} (30\%):
\begin{itemize}
    \item Baseline model training and evaluation
    \item Statistical analysis and visualization
    \item Literature review and background research
    \item Report writing and presentation
\end{itemize}

All team members contributed to project planning, weekly meetings, debugging sessions, and manuscript preparation.

\section{Discussion and Future Directions}

\subsection{Addressing Assumptions and Challenges}

\textbf{Assumption 1}: Temporal patterns in laboratory values are predictive of readmission.

\textit{Evidence}: LSTM achieves 0.609 AUROC, demonstrating that temporal modeling provides value despite data sparsity. However, improvement is modest (+0.003 over baseline), suggesting diminishing returns with current feature set.

\textbf{Assumption 2}: Deep learning models can handle missing data through masking.

\textit{Challenge}: Extreme sparsity (97\% missing) limits model capacity to learn temporal patterns. Many patients have only 1-2 observed timepoints, insufficient for meaningful sequence modeling.

\textit{Future Work}: Investigate imputation strategies (forward-fill, interpolation) vs. explicit missingness indicators. Consider incorporating missingness patterns as features (e.g., frequency of lab ordering may indicate illness severity).

\textbf{Assumption 3}: Attention mechanisms improve interpretability.

\textit{Challenge}: With sparse data, attention weights may be unreliable or uninformative. Preliminary experiments show that Transformer attention often focuses on the few observed timepoints by default, providing limited insight.

\textit{Future Work}: Develop alternative interpretability methods tailored to sparse data (e.g., temporal saliency maps, counterfactual analysis).

\subsection{Broader Impact}

Accurate readmission prediction could enable:
\begin{itemize}
    \item \textbf{Clinical Decision Support}: Alert clinicians to high-risk patients requiring intensive discharge planning or early follow-up.
    \item \textbf{Resource Optimization}: Allocate limited post-discharge resources (home health, care coordination) to patients most likely to benefit.
    \item \textbf{Value-Based Care}: Reduce preventable readmissions, improving quality metrics and reducing costs under bundled payment models.
\end{itemize}

However, deployment requires careful consideration of:
\begin{itemize}
    \item \textbf{Fairness}: Ensure models do not discriminate against disadvantaged populations (racial minorities, uninsured).
    \item \textbf{Calibration}: Predicted probabilities must be reliable for clinical decision-making.
    \item \textbf{Integration}: Model predictions must fit into existing workflows without increasing clinician burden.
\end{itemize}

\section{Conclusion}

This milestone report presents our progress toward developing temporal deep learning models for 30-day hospital readmission prediction. We have:
\begin{enumerate}
    \item Built a complete data pipeline processing 225,323 admissions from MIMIC-IV
    \item Established strong baselines (Random Forest: 0.610 AUROC)
    \item Developed and debugged LSTM (preliminary: 0.609 AUROC) and Transformer models
    \item Addressed critical technical challenges (data sparsity, numerical stability)
\end{enumerate}

Preliminary results demonstrate that temporal modeling can match or slightly exceed traditional approaches, even with extreme data sparsity. Our remaining work focuses on finalizing model training, conducting statistical evaluation, and performing causal inference to answer policy-relevant questions about discharge timing.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{9}

\bibitem{jencks2009}
S. F. Jencks, M. V. Williams, and E. A. Coleman,
``Rehospitalizations among patients in the Medicare fee-for-service program,''
\textit{New England Journal of Medicine}, vol. 360, no. 14, pp. 1418--1428, 2009.

\bibitem{kansagara2011}
D. Kansagara et al.,
``Risk prediction models for hospital readmission: A systematic review,''
\textit{JAMA}, vol. 306, no. 15, pp. 1688--1698, 2011.

\bibitem{johnson2023mimic}
A. E. W. Johnson et al.,
``MIMIC-IV, a freely accessible electronic health record dataset,''
\textit{Scientific Data}, vol. 10, no. 1, 2023.

\end{thebibliography}

\end{document}
